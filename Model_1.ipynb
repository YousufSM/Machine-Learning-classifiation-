{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:11:59.617616Z",
     "start_time": "2020-07-16T14:11:59.510677Z"
    }
   },
   "outputs": [],
   "source": [
    "def fn_logistic(df_train,df_test):\n",
    "\n",
    "    # Importing libraries\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import h2o\n",
    "    from h2o.estimators import H2OGeneralizedLinearEstimator\n",
    "    from h2o.grid.grid_search import H2OGridSearch\n",
    "    ! pip install category_encoders # This is to use Target Encoders. This will update Sk-learn as well\n",
    "    from category_encoders import TargetEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ------------------------------------------------\n",
    "\n",
    "    DEFINING VARIABLES\n",
    "\n",
    "    ------------------------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    vars_ind_numeric = ['a04', 'a05','a06','a07','a08','a09','a11','a14','a15','b01','b05','b06','c01','c03','d01','d02','d03',\n",
    "    'e02','e04','e05','e06','e07','e08','e09','e12','e15','e16','e23','f01','f02','f06','f08','f11','f13','f15','f16','f17','f18',\n",
    "    'f19','f20','f21','f22','f23','f24','f25','f26','f28','f31','f32','unique_id']\n",
    "\n",
    "    vars_ind_categorical = ['a01','a02','a03','a10','a12','a13','a16','a17','a18','a19','a20','b02','b03','b04','b07','c02','c04',\n",
    "     'c05','c06','c07','c08','c09','e01','e03','e11','e13','e14','e21','e22','e24','e25','e17','e18','e19','e20','f03','f04','f05',\n",
    "     'f07', 'f09','f27','f29','f30','f33','f34','f10']\n",
    "\n",
    "    vars_notToUse = ['unique_id']\n",
    "\n",
    "    var_dep = ['target']\n",
    "\n",
    "    vars_ind_hccv = ['e17', 'e18', 'e19', 'f10']\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ------------------------------------------------\n",
    "\n",
    "    ######################################################################################\n",
    "    SPLINES:\n",
    "    ######################################################################################\n",
    "\n",
    "    Splines have been created for three variables ['f02', 'f11', 'f13'].\n",
    "    I ran a Random Forest algorithm considering only numeric variables to see which all numeric variables are the most important.\n",
    "    I did not make splines on all numeric variables, as making splines for all variables could not improve the \n",
    "    performance significantly but made the model quite complex to comprehend. Therefore, it was a trade-off between \n",
    "    accuracy and complexity.\n",
    "\n",
    "    For running the random forest, I made a \"Sample\" dataframe and removed -99 (NAs). I have treated -99 on train and test dataframe\n",
    "    afterwards through H20. The reason why I did not treat NAs right now was because few of NAs are being taken as NAs in H20 while few of them were \n",
    "    been taken as \"NA\" (String) which was strange. Therefore I treated all NAs in H20 only.\n",
    "\n",
    "    Also, I did not standarised the variables, because Random Forest (for splines) do not require standardising variables \n",
    "    while H20 will automatically treat them while doing Logistic Regression.\n",
    "\n",
    "    I have run the spline function on test as \"fn_tosplines(df_test[var])\" because what I feels is, in real life scenario, we\n",
    "    wouldn't be having the test frame and therefore have to use the pre-decided variables and percentile values on test frame.\n",
    "\n",
    "\n",
    "\n",
    "    ######## The Following is the code that gave us most important numeric feature through Random Forest ########\n",
    "\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from matplotlib import pyplot\n",
    "\n",
    "    sample = df_train.copy() \n",
    "    sample = sample.replace(-99, np.nan)\n",
    "    sample = sample.dropna()\n",
    "\n",
    "    # random forest for feature importance on a classification problem\n",
    "    # define dataset\n",
    "    X, y = sample[[var for var in vars_ind_numeric if var not in vars_notToUse]], np.array(sample[var_dep])\n",
    "    # define the model\n",
    "    model = RandomForestClassifier()\n",
    "    # fit the model\n",
    "    model.fit(X, y)\n",
    "    # get importance\n",
    "    importance = model.feature_importances_\n",
    "    # summarize feature importance\n",
    "    for i,v in enumerate(importance):\n",
    "        print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    # plot feature importance\n",
    "    pyplot.bar([x for x in range(len(importance))], importance)\n",
    "    pyplot.show()\n",
    "\n",
    "    high_imprtance_num_var = list(X.columns)\n",
    "    high_imprtance_num_var  = [high_imprtance_num_var [i] for i in [29,32,33]]\n",
    "\n",
    "    ################## END ##################\n",
    "\n",
    "    ------------------------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "    high_imprtance_num_var = ['f02', 'f11', 'f13']\n",
    "\n",
    "\n",
    "    vars_ind_tospline = high_imprtance_num_var\n",
    "\n",
    "    def fn_tosplines(x):\n",
    "        x = x.values\n",
    "        # hack: remove zeros to avoid issues where lots of values are zero\n",
    "        x_nonzero = x[x != 0]\n",
    "        ptiles = np.percentile(x_nonzero, [10, 20, 40, 60, 80, 90])\n",
    "        #print(var, ptiles)\n",
    "        df_ptiles = pd.DataFrame({var: x})\n",
    "        for idx, ptile in enumerate(ptiles):\n",
    "            df_ptiles[var + '_' + str(idx)] = np.maximum(0, x - ptiles[idx])\n",
    "        return(df_ptiles)\n",
    "\n",
    "    ## Splines for Train set\n",
    "    for var in vars_ind_tospline:\n",
    "        df_ptiles = fn_tosplines(df_train[var])\n",
    "        df_train.drop(columns=[var], inplace=True)\n",
    "        vars_ind_numeric.remove(var)\n",
    "        df_train = pd.concat([df_train, df_ptiles], axis=1, sort=False)\n",
    "        vars_ind_numeric.extend(df_ptiles.columns.tolist())\n",
    "\n",
    "\n",
    "    ## Splines for Test set\n",
    "    for var in vars_ind_tospline:\n",
    "        df_ptiles_t = fn_tosplines(df_test[var])  # Splines on train set has been used\n",
    "        df_test.drop(columns=[var], inplace=True)\n",
    "        df_test = pd.concat([df_test, df_ptiles_t], axis=1, sort=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ------------------------------------------------\n",
    "    ######################################################################################\n",
    "    TREATING NAs : Justification for using Mean imputes instead of Skip method:\n",
    "    ######################################################################################\n",
    "    \n",
    "    H20 documentation for GLM suggest that it automatically mean imputes the NAs while the another method is removing \n",
    "    the observations that have NAs. The document also suggest that if, we have few columns with many NAs, we might accidentally \n",
    "    be losing all our rows so its better to exclude (skip) them. While on the other hand, if we have many columns with a small\n",
    "    fraction of uniformly distributed missing values where every row is likely to have at least one missing value. In this case, \n",
    "    impute the NAs is suggested (e.g., substitute the NAs with mean values) before modeling.\n",
    "\n",
    "    Through our code (attached below) we can see in the test frame there are many rows with few NAs and thus if we use Skip\n",
    "    method as we might loose information. \n",
    "    Therefore we have used Mean imputation method (Although Skip method showed a better accuracy on Train frame).\n",
    "\n",
    "    NAs will be treated in the H20 Frame, right now we are only removing 'C02' variable as it has got >50% of NAs.\n",
    "\n",
    "\n",
    "\n",
    "    ################## Code ##################\n",
    "\n",
    "    # To search if there are any categorical variables with NAs where NA == -99\n",
    "    (df_train[vars_ind_numeric] == -99).astype(int).sum(axis=0)\n",
    "    # Yes there are a lot of variables with NAs, in some of them NAs are near to 100,000 values\n",
    "\n",
    "    # To search if there are any categorical variables with NAs\n",
    "    df_test[vars_ind_categorical].isna().sum(axis = 0)\n",
    "    # Yes, there are a lot of categorical varibales with NAs\n",
    "\n",
    "    ################## END ##################\n",
    "\n",
    "    ------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Since C02 has a lot of NAs (>50%), we will drop it from both train and test\n",
    "    df_test.drop('c02', axis=1, inplace = True)\n",
    "    df_train.drop('c02', axis=1, inplace = True)\n",
    "    vars_ind_categorical.remove('c02')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ------------------------------------------------\n",
    "    ######################################################################################\n",
    "    CARDINALITY: \n",
    "    ######################################################################################\n",
    "    \n",
    "    For treating the hccv we have used Target Encoders (Sk-Learn). We have also observed that there is oversampling of few factors \n",
    "    while others are under-sampled therefore we have used smoothing factor = 4. Smoothing of 4 is chosen by following few online \n",
    "    blogs. Also scikit learn is used to do the target encoding because I was a bit confused with H20 target encoding. Additionally,\n",
    "    H20 automatically perform one_hot encoding on categorical variables so they have not been treated.\n",
    "    Note: I needed to update the scikit-learn and category_ecoders library\n",
    "\n",
    "    ------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Target encoders on Train\n",
    "    enc = TargetEncoder(cols=vars_ind_hccv, smoothing =4)\n",
    "    enc.fit_transform(df_train, df_train['target'])\n",
    "    df_train = enc.transform(df_train, df_train['target'])\n",
    "\n",
    "\n",
    "    # Target encoders on Test\n",
    "    df_test['target'] = np.nan # Creating dummy 'target' variable for using the the enc.transform function\n",
    "    df_test = enc.transform(df_test) # applying the already trained encoder\n",
    "    df_test.drop(columns=['target'], inplace=True) # Dropping dummy 'target' variable\n",
    "\n",
    "\n",
    "\n",
    "    # Initiating h20\n",
    "    # h2o.init(port=54321)\n",
    "    h2o.init(port=54321, max_mem_size = \"14g\") # Asking h20 to use 14 GB of Ram\n",
    "    h2o.connect()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ------------------------------------------------\n",
    "    ######################################################################################\n",
    "    H20 FRAMES:\n",
    "    ######################################################################################\n",
    "    We have removed 'unique_id' from Frames as it was not useful in prediction\n",
    "\n",
    "    ------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # H20 Frames: \n",
    "    vars_to_use = vars_ind_numeric + vars_ind_categorical\n",
    "    vars_to_use.remove('unique_id')\n",
    "    vars_ind_numeric.remove('unique_id')\n",
    "\n",
    "\n",
    "    h2o_df_train = h2o.H2OFrame(df_train[[var for var in vars_to_use+var_dep ]], destination_frame = 'df_train') # Train Frame\n",
    "    h2o_df_test  = h2o.H2OFrame(df_test[[var for var in vars_to_use]], destination_frame = 'df_test')  # Test Frame\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ------------------------------------------------\n",
    "    ######################################################################################\n",
    "    TREATING NAs:\n",
    "    ######################################################################################\n",
    "    \n",
    "    Converting -99 to NAs. We have not done this before becuase as discussed earlier, converting pandas NAs to H20 Frame\n",
    "    were not consistent and was giving an error (may be due to the older version of H20 in the image).\n",
    "\n",
    "    ------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Converting -99 to NA in train\n",
    "    for var in vars_ind_numeric:\n",
    "        h2o_df_train[h2o_df_train[var] == -99.0 , var] = None\n",
    "\n",
    "\n",
    "    # Converting -99 to NA in test\n",
    "    for var in vars_ind_numeric:\n",
    "        h2o_df_test[h2o_df_test[var] == -99.0 , var] = None\n",
    "\n",
    "\n",
    "\n",
    "    # H20 Document suggest to make dependent variable as factor for classification task\n",
    "    h2o_df_train[var_dep] = h2o_df_train[var_dep].asfactor()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ------------------------------------------------\n",
    "    ######################################################################################\n",
    "    INTERACTIONS:\n",
    "    ######################################################################################\n",
    "    \n",
    "    Calculating glm with interactions increases the kaggle score from 77% to 83%.\n",
    "\n",
    "    I have not used the interaction on all variables, instead I have used it only on 2 variables ('f03' and 'e11'). These two \n",
    "    variables have been chosen after running GLM model and then calculating variables importance.\n",
    "    I did not use all categorical variables, as it could not improve the performance significantly but made the\n",
    "    model quite complex to comprehend. Therefore, it was a trade-off between accuracy and complexity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ################## CODE ##################\n",
    "\n",
    "    # Code for running GLM to see which all variables are important\n",
    "\n",
    "    Idea has been taken from: https://aichamp.wordpress.com/2017/09/29/python-example-of-building-glm-gbm-and-random-forest-\n",
    "    binomial-model-with-h2o/\n",
    "\n",
    "\n",
    "    from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "    glm_logistic = H2OGeneralizedLinearEstimator(family = \"binomial\")\n",
    "    glm_logistic.train(x=vars_to_use , y= 'target', training_frame=h2o_df_train, model_id=\"glm_logistic\")\n",
    "    preds = glm_logistic.predict(h2o_df_test)\n",
    "    df_test['Predicted'] = np.round(preds[2].as_data_frame(), 5)\n",
    "    df_preds_dt = df_test[['unique_id', 'Predicted']].copy()\n",
    "    df_test[['unique_id', 'Predicted']].to_csv(dirPOutput + '1st.csv', index=False)\n",
    "    log_var_imp = glm_logistic.varimp(use_pandas=True).head()\n",
    "    log_var_imp.loc[0:5, 'variable'].tolist()\n",
    "\n",
    "    Output : ['f10', 'f03.F', 'e19', 'e11.A', 'f03.E']\n",
    "\n",
    "    ################## END ##################\n",
    "\n",
    "\n",
    "    ------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # I have chosen min_occurence as int(len(h2o_df_train)/40) after many trial and error.\n",
    "    # With int(len(h2o_df_train)/40) on 250K train data I was getting 5 factors for each variable. I believe performing \n",
    "    # interactions on top 4-5 variables rather than all the variables having occurrence > 10/20 would make the model\n",
    "    # faster and more interpretable\n",
    "    \n",
    "    # Train Frame\n",
    "    interaction_frame_train = h2o_df_train.interaction(['f03', 'e11'], pairwise = False, max_factors = 100,\n",
    "                                                       min_occurrence = int(len(h2o_df_train)/40))\n",
    "    # Test Frame\n",
    "    interaction_frame_test = h2o_df_test.interaction(['f03', 'e11'], pairwise = False, max_factors = 100, \n",
    "                                                     min_occurrence = int(len(h2o_df_train)/40))\n",
    "\n",
    "\n",
    "    # Cbinding interaction frame to train and test\n",
    "    h2o_df_train = h2o_df_train.cbind(interaction_frame_train)\n",
    "    h2o_df_test = h2o_df_test.cbind(interaction_frame_test)\n",
    "\n",
    "    # incuding interaction frame's variable to variables list\n",
    "    vars_to_use = vars_to_use + ['f03_e11']\n",
    "    vars_ind_numeric = vars_ind_numeric + ['f03_e11']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ------------------------------------------------\n",
    "\n",
    "\n",
    "    ######################################################################################\n",
    "    Running Grid Search for hyper parameters\n",
    "    ######################################################################################\n",
    "    The thought process of selecting each parameters and its values has been discussed below\n",
    "    \n",
    "    # * We could have added 'missing_values_handling': [\"skip\", \"mean_imputation\"] to the  hyper-parameters, but as discussed\n",
    "    above, predicting with 'Skip' method may give us a better accuracy on train, but will give us a bad result on Test\n",
    "    # * We have not included the 'Standarised' parameter because it is True by default and wants to keep it that way.\n",
    "    # * Search Strategy: The default strategy, \"Cartesian\", covers the entire space of h-p combinations but takes too much time.\n",
    "    Therefore we have used \"Random Discrete Strategy\".\n",
    "    # * \"max_models\": 30: taken the idea of 30 models from the lecture notes\n",
    "    # * stopping_metric\": \"AUTO\": Auto is logloss for classification. For task 2 & 3, stopping_metric is set to AUC\n",
    "    # * seed = 2020: Seed so that it generates similar model every-time\n",
    "    # * nfolds = 5: nfolds ideally took more time but I believe it is more efficient in terms of training the algorithm\n",
    "    rather than taking validation set.\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Defining hyper parameters to search for\n",
    "\n",
    "    alpha_opts = np.arange(0, 1, 0.01).tolist()\n",
    "    hyper_parameters = {\"alpha\":alpha_opts} \n",
    "\n",
    "\n",
    "    # Defining criteria\n",
    "    criteria = {\"strategy\": \"RandomDiscrete\", \n",
    "                \"max_runtime_secs\": 5400,\n",
    "                \"max_models\": 30, \n",
    "                \"stopping_metric\": \"AUTO\", \n",
    "                \"seed\": 2020}\n",
    "\n",
    "    # Model\n",
    "    grid = H2OGridSearch(H2OGeneralizedLinearEstimator(family=\"binomial\", # Logit link function\n",
    "                                                       nfolds = 5, \n",
    "                                                       lambda_search=True), \n",
    "                         hyper_params=hyper_parameters,\n",
    "                         grid_id='g1',\n",
    "                         search_criteria=criteria)\n",
    "\n",
    "\n",
    "    # Training the model\n",
    "    grid.train(y = \"target\",\n",
    "               x = vars_to_use, \n",
    "               training_frame = h2o_df_train\n",
    "               )\n",
    "\n",
    "\n",
    "    # Looking the grid results\n",
    "    grid = grid.get_grid(sort_by='accuracy', decreasing=True)\n",
    "    bst = grid.models[1] # check if you want 0 or 1\n",
    "    grid\n",
    "\n",
    "\n",
    "    # Observing which all variants are the most important for my own understanding. These co-efficient are actually different\n",
    "    # from what we got from our previous GLM and Random Forest method except f03 is common\n",
    "    # Normalised coefficients\n",
    "    bst_coef = bst.coef_norm()\n",
    "    # {k: v for k, v in sorted(bst_coef.items(), key=lambda item: item[1])}\n",
    "\n",
    "    # Output: f10, e19, f03, f27, f29\n",
    "\n",
    "    ------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ------------------------------------------------\n",
    "\n",
    "    ######################################################################################\n",
    "    Training and fitting the H2OGeneralizedLinearEstimator (GLM)\n",
    "    ######################################################################################\n",
    "\n",
    "    The grid search returned 3 models in 90 minutes. We could have waited for the entire process of getting 30 models\n",
    "    but I didn't as it would have taken 15 hours to do so but I wonder we might get a better result.\n",
    "\n",
    "    The model returned by the Grid search is as follow and the best model is having alpha = 0.92\n",
    "\n",
    "          alpha   model_ids             accuracy\n",
    "    0    [0.92]  g1_model_1             0.745579\n",
    "    1    [0.49]  g1_model_2   0.7443500000000001\n",
    "    2    [0.01]  g1_model_3  0.49104800000000004\n",
    "\n",
    "\n",
    "    ------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    GLM_model = H2OGeneralizedLinearEstimator(family=\"binomial\", # Logit link function\n",
    "                                  nfolds = 5, # nfolds ideally took more time but I believe it is the correct \n",
    "                                  # way of doing rather than taking validation set.\n",
    "                                  lambda_search=True, # Lambda is a regularisation object and lambda_search \n",
    "                                  alpha = 0.92, # As returned by the grid search\n",
    "                                  seed = 2020 ,\n",
    "                                  max_runtime_secs = 5400, # allowed to run for 90 mins\n",
    "                                  )\n",
    "\n",
    "\n",
    "    GLM_model.train(y = \"target\",\n",
    "               x = vars_to_use, \n",
    "               training_frame = h2o_df_train\n",
    "               )\n",
    "    \n",
    "    \n",
    "    # Predicting the test values\n",
    "    preds = GLM_model.predict(h2o_df_test) # previously written grid\n",
    "    df_test['Predicted'] = np.round(preds[2].as_data_frame(), 5)\n",
    "    df_preds_dt = df_test[['unique_id', 'Predicted']].copy()\n",
    "    result_df_test = df_test[['unique_id', 'Predicted']]\n",
    "\n",
    "\n",
    "    return(GLM_model, result_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_logistic(df_train,df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
