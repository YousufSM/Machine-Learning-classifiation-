{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing AutoML (H20)\n",
    "- modeltype: **AutoML** (H20)\n",
    "- train: **0.912**\n",
    "- test: **0.860**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: TPOT has been implemented in another notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen Auto ML for part three as I was curious about autoML and have been reading about it over the internet for the past few weeks. Thus wanted to get my hands on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow of the notebook\n",
    "* 1. Reading Data and importing libraries\n",
    "* 2. Data Manipulation\n",
    "* 3. Implementing AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "import h2o\n",
    "from h2o.estimators import H2OGeneralizedLinearEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "from category_encoders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install category-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Final_Project/Pcode\n"
     ]
    }
   ],
   "source": [
    "# Set directories\n",
    "print(os.getcwd())\n",
    "dirRawData = \"../input/\"\n",
    "dirPData   = \"../PData/\"\n",
    "dirPOutput = \"../POutput/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = dirPData + '01_df_250k.pickle'\n",
    "\n",
    "with (open(f_name, \"rb\")) as f:\n",
    "    dict_ = pickle.load(f)\n",
    "\n",
    "df_train = dict_['df_train']\n",
    "df_test  = dict_['df_test']\n",
    "\n",
    "del f_name, dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = dirPData + '01_vars.pickle'\n",
    "\n",
    "with open(f_name, \"rb\") as f:\n",
    "    dict_ = pickle.load(f)\n",
    "\n",
    "vars_ind_numeric     = dict_['vars_ind_numeric']\n",
    "vars_ind_hccv        = dict_['vars_ind_hccv']\n",
    "vars_ind_categorical = dict_['vars_ind_categorical']\n",
    "vars_notToUse        = ['unique_id']\n",
    "var_dep              = dict_['var_dep']\n",
    "\n",
    "del f_name, dict_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T17:08:00.680817Z",
     "start_time": "2020-07-16T17:08:00.667823Z"
    }
   },
   "source": [
    "Since AutoML also explores GLM, I incuded splines to make the model more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T17:07:27.953303Z",
     "start_time": "2020-07-16T17:07:27.840367Z"
    }
   },
   "source": [
    "Splines have been created for three varibales ['f02', 'f11', 'f13'].\n",
    "I ran a Random Forest algorithm considering only numeric variables to see which all numeric variables are the most important.\n",
    "I did not want to make splines on all numeric variables as it may decrease my speed exponentially by only giving a slightly \n",
    "better accuracy.\n",
    "\n",
    "For running the random forest, I made a \"Sample\" dataframe and removed -99 (NAs). I have treated -99 on train and test dataframe\n",
    "afterwards through H20. The reason why I did not treat NAs right now was because few of NAs are being taken as NAs in H20 while few of them were \n",
    "been taken as \"NA\" (String) which was strange. Therefore I treated all NAs in H20 only.\n",
    "\n",
    "Also, I did not standarised the variables, because Random Forest (for splines) do not require standarise variables \n",
    "while H20 will automatically treat them while doing Logistic Regression.\n",
    "\n",
    "I have run the spline function on test as \"fn_tosplines(df_test[var])\" becasue what I feels is, in real life scenario, we\n",
    "wouldn't be having the test frame and therfore have to use the pre-decided variables and percentile values on test frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.03598\n",
      "Feature: 1, Score: 0.00165\n",
      "Feature: 2, Score: 0.00343\n",
      "Feature: 3, Score: 0.00099\n",
      "Feature: 4, Score: 0.00172\n",
      "Feature: 5, Score: 0.00393\n",
      "Feature: 6, Score: 0.00128\n",
      "Feature: 7, Score: 0.01668\n",
      "Feature: 8, Score: 0.02994\n",
      "Feature: 9, Score: 0.04107\n",
      "Feature: 10, Score: 0.04188\n",
      "Feature: 11, Score: 0.01643\n",
      "Feature: 12, Score: 0.02677\n",
      "Feature: 13, Score: 0.02518\n",
      "Feature: 14, Score: 0.03268\n",
      "Feature: 15, Score: 0.00675\n",
      "Feature: 16, Score: 0.01379\n",
      "Feature: 17, Score: 0.05130\n",
      "Feature: 18, Score: 0.04107\n",
      "Feature: 19, Score: 0.04940\n",
      "Feature: 20, Score: 0.02491\n",
      "Feature: 21, Score: 0.00312\n",
      "Feature: 22, Score: 0.02837\n",
      "Feature: 23, Score: 0.02781\n",
      "Feature: 24, Score: 0.05278\n",
      "Feature: 25, Score: 0.04322\n",
      "Feature: 26, Score: 0.00000\n",
      "Feature: 27, Score: 0.02230\n",
      "Feature: 28, Score: 0.02919\n",
      "Feature: 29, Score: 0.10226\n",
      "Feature: 30, Score: 0.01353\n",
      "Feature: 31, Score: 0.00134\n",
      "Feature: 32, Score: 0.07993\n",
      "Feature: 33, Score: 0.07176\n",
      "Feature: 34, Score: 0.01039\n",
      "Feature: 35, Score: 0.00658\n",
      "Feature: 36, Score: 0.00472\n",
      "Feature: 37, Score: 0.00136\n",
      "Feature: 38, Score: 0.00176\n",
      "Feature: 39, Score: 0.00647\n",
      "Feature: 40, Score: 0.00080\n",
      "Feature: 41, Score: 0.00386\n",
      "Feature: 42, Score: 0.00027\n",
      "Feature: 43, Score: 0.00062\n",
      "Feature: 44, Score: 0.00054\n",
      "Feature: 45, Score: 0.00094\n",
      "Feature: 46, Score: 0.00130\n",
      "Feature: 47, Score: 0.00899\n",
      "Feature: 48, Score: 0.00895\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######## The Following is the code that gave us most important numeric feature through Random Forest ########\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "sample = df_train.copy() \n",
    "sample = sample.replace(-99, np.nan)\n",
    "sample = sample.dropna()\n",
    "\n",
    "# random forest for feature importance on a classification problem\n",
    "# define dataset\n",
    "X, y = sample[[var for var in vars_ind_numeric if var not in vars_notToUse]], np.array(sample[var_dep])\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "high_imprtance_num_var = list(X.columns)\n",
    "high_imprtance_num_var  = [high_imprtance_num_var [i] for i in [29,32,33]]\n",
    "\n",
    "################## END ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_imprtance_num_var = ['f02', 'f11', 'f13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_ind_tospline = high_imprtance_num_var\n",
    "\n",
    "def fn_tosplines(x):\n",
    "    x = x.values\n",
    "    # hack: remove zeros to avoid issues where lots of values are zero\n",
    "    x_nonzero = x[x != 0]\n",
    "    ptiles = np.percentile(x_nonzero, [10, 20, 40, 60, 80, 90])\n",
    "    #print(var, ptiles)\n",
    "    df_ptiles = pd.DataFrame({var: x})\n",
    "    for idx, ptile in enumerate(ptiles):\n",
    "        df_ptiles[var + '_' + str(idx)] = np.maximum(0, x - ptiles[idx])\n",
    "    return(df_ptiles)\n",
    "\n",
    "for var in vars_ind_tospline:\n",
    "    df_ptiles = fn_tosplines(df_train[var])\n",
    "    df_train.drop(columns=[var], inplace=True)\n",
    "    vars_ind_numeric.remove(var)\n",
    "    df_train = pd.concat([df_train, df_ptiles], axis=1, sort=False)\n",
    "    vars_ind_numeric.extend(df_ptiles.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in vars_ind_tospline:\n",
    "    df_ptiles_t = fn_tosplines(df_train[var])  # here we use the same ptiles as for train to spline the same way\n",
    "    df_test.drop(columns=[var], inplace=True)\n",
    "    # vars_ind_numeric_test.remove(var)\n",
    "    df_test = pd.concat([df_test, df_ptiles_t], axis=1, sort=False)\n",
    "    # vars_ind_numeric_test.extend(df_ptiles_t.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in earlier questions, H20 takes care of missing values in both GLM and XGBoost method therefore here we are only deleting variable 'c02' since it has >50% NAs in test data. -99 values will be treated in the H20 Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Since C02 has a lot of NAs (>50%), we will drop it from both train and test\n",
    "df_test.drop('c02', axis=1, inplace = True)\n",
    "df_train.drop('c02', axis=1, inplace = True)\n",
    "vars_ind_categorical.remove('c02')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Cardinality\n",
    "   For treating the hccv we have used Target Encoders (Sk-Learn). We have also observed that there is oversampling of few factors \n",
    "    while others are under-sampled therefore we have used smoothing factor = 4. Smoothing of 4 is chosen by following few online \n",
    "    blogs. Also scikit learn is used to do the target encoding because I was a bit confused with H20 target encoding. Additionally,\n",
    "    H20 automatically perform one_hot encoding on categorical variables so they have not been treated.\n",
    "\n",
    "\n",
    "Note: I needed to update the scikit-learn and category_ecoders library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the distribution of factors in a hcc variable\n",
    "# df_train[vars_ind_hccv].nunique()\n",
    "# df_train['e18'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoders on Train\n",
    "enc = TargetEncoder(cols=vars_ind_hccv, smoothing =4)\n",
    "enc.fit_transform(df_train, df_train['target'])\n",
    "df_train = enc.transform(df_train, df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoders on Test\n",
    "df_test['target'] = np.nan # Creating dummy 'target' variable for using the the enc.transform function\n",
    "df_test = enc.transform(df_test) # applying the already trained encoder\n",
    "df_test.drop(columns=['target'], inplace=True) # Dropping dummy 'target' variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running H20 cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shutdown'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8ae2ce013486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shutdown'"
     ]
    }
   ],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n",
      "Warning: Your H2O cluster version is too old (1 year, 2 months and 8 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>1 hour 24 mins</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 year, 2 months and 8 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_jovyan_s0rtq1</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>12.24 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.7 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         1 hour 24 mins\n",
       "H2O cluster timezone:       Etc/UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.3\n",
       "H2O cluster version age:    1 year, 2 months and 8 days !!!\n",
       "H2O cluster name:           H2O_from_python_jovyan_s0rtq1\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    12.24 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.7 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://localhost:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>1 hour 24 mins</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 year, 2 months and 8 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_jovyan_s0rtq1</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>12.24 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.7 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         1 hour 24 mins\n",
       "H2O cluster timezone:       Etc/UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.3\n",
       "H2O cluster version age:    1 year, 2 months and 8 days !!!\n",
       "H2O cluster name:           H2O_from_python_jovyan_s0rtq1\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    12.24 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.7 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<H2OConnection to http://localhost:54321, no session>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o.init(port=54321, max_mem_size = \"14g\") # Asking h20 to use 14 GB of Ram\n",
    "h2o.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have removed 'unique_id' from Frames as it was not useful in prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "vars_to_use = vars_ind_numeric + vars_ind_categorical\n",
    "vars_to_use.remove('unique_id')\n",
    "vars_ind_numeric.remove('unique_id')\n",
    "\n",
    "\n",
    "h2o_df_train = h2o.H2OFrame(df_train[[var for var in vars_to_use+var_dep ]], destination_frame = 'df_train') # Train Frame\n",
    "h2o_df_test  = h2o.H2OFrame(df_test[[var for var in vars_to_use]], destination_frame = 'df_test') # Test Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting -99 to NAs. We have not done this before becuase as discussed earlier, converting pandas NAs to H20 Frame were not consistent and was giving an error (may be due to the older version of H20 in the image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting -99 to NA in train\n",
    "for var in vars_ind_numeric:\n",
    "    h2o_df_train[h2o_df_train[var] == -99.0 , var] = None\n",
    "    \n",
    "# Converting -99 to NA in test\n",
    "for var in vars_ind_numeric:\n",
    "    h2o_df_test[h2o_df_test[var] == -99.0 , var] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H20 Document suggest to make dependant variable as factor for classiication task\n",
    "h2o_df_train[var_dep] = h2o_df_train[var_dep].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Interactions\n",
    "Documentation regarding the interaction in XGBoost was not clear, therefore after  searching for online, especially [here](https://www.kaggle.com/c/bosch-production-line-performance/discussion/24418) making interaction seemed to be a logical choice given it has drastically improved the past model (GLM) performance.\n",
    "\n",
    "I have not used the interaction on all variables, instead I have used it only on 2 variables ('f03' and 'e11'). These two \n",
    "variables have been chosen after running GLM model and then calculating variables importance.\n",
    "I did not use all categorical variables, as it could not improve the performance significantly but made the\n",
    "model quite complex to comprehend. Therefore, it was a trade-off between accuracy and complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "glm prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'e20' has levels not trained on: [30146, BE271, nan]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'e13' has levels not trained on: [Q, S]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'e24' has levels not trained on: [J, M, nan]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'e03' has levels not trained on: [J, nan]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'c09' has levels not trained on: [nan]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'a18' has levels not trained on: [D]\n",
      "  warnings.warn(w)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['f10', 'f03.F', 'e19', 'e11.A', 'f03.E']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for runing GLM to see which all variables are important\n",
    "\n",
    "# Idea has been taken from: https://aichamp.wordpress.com/2017/09/29/python-example-of-building-glm-gbm-and-random-forest-\n",
    "# binomial-model-with-h2o/\n",
    "\n",
    "\n",
    "# from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "# glm_logistic = H2OGeneralizedLinearEstimator(family = \"binomial\")\n",
    "# glm_logistic.train(x=vars_to_use , y= 'target', training_frame=h2o_df_train, model_id=\"glm_logistic\")\n",
    "# preds = glm_logistic.predict(h2o_df_test)\n",
    "# df_test['Predicted'] = np.round(preds[2].as_data_frame(), 5)\n",
    "# df_preds_dt = df_test[['unique_id', 'Predicted']].copy()\n",
    "# df_test[['unique_id', 'Predicted']].to_csv(dirPOutput + '1st.csv', index=False)\n",
    "# log_var_imp = glm_logistic.varimp(use_pandas=True).head()\n",
    "# log_var_imp.loc[0:5, 'variable'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen min_occurence as int(len(h2o_df_train)/40) after many trial and error.\n",
    "\n",
    "With int(len(h2o_df_train)/40) on 250K train data I was getting 5 factors for each variable. I believe performing \n",
    "interactions on top 4-5 variables rather than all the variables having occurrence > 10/20 would make the model\n",
    "faster and more interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions progress: |██████████████████████████████████████████████████| 100%\n",
      "Interactions progress: |██████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Train Frame\n",
    "interaction_frame_train = h2o_df_train.interaction(['f03', 'e11'], pairwise = False, max_factors = 100,\n",
    "                                                   min_occurrence = int(len(h2o_df_train)/40))\n",
    "\n",
    "# Test Frame\n",
    "interaction_frame_test = h2o_df_test.interaction(['f03', 'e11'], pairwise = False, max_factors = 100, \n",
    "                                                 min_occurrence = int(len(h2o_df_train)/40))\n",
    "\n",
    "# Cbinding interaction frame to train and test\n",
    "h2o_df_train = h2o_df_train.cbind(interaction_frame_train)\n",
    "h2o_df_test = h2o_df_test.cbind(interaction_frame_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incuding interaction frame's variable to variables list\n",
    "vars_to_use = vars_to_use + ['f03_e11']\n",
    "vars_ind_numeric = vars_ind_numeric + ['f03_e11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving this data to be used for TPOT analysis\n",
    "\n",
    "train_for_TPOT = h2o.as_list(h2o_df_train)\n",
    "test_for_TPOT = h2o.as_list(h2o_df_test)\n",
    "\n",
    "dict_ = {'df_train': train_for_TPOT,\n",
    "         'df_test': test_for_TPOT}\n",
    "\n",
    "f_name = dirPData + 'TPOT_DB.pickle'\n",
    "with open(f_name, \"wb\") as f:\n",
    "    pickle.dump(dict_, f)\n",
    "del f_name, dict_\n",
    "\n",
    "# store variable names to pickle\n",
    "dict_ = {'vars_ind_numeric': vars_ind_numeric,\n",
    "         'vars_to_use': vars_to_use,\n",
    "         'var_dep': var_dep}\n",
    "\n",
    "f_name = dirPData + 'TPOT_VARS.pickle'\n",
    "with open(f_name, \"wb\") as f:\n",
    "    pickle.dump(dict_, f)\n",
    "del f_name, dict_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing AutoML in H20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T18:16:15.943180Z",
     "start_time": "2020-07-16T18:16:15.934184Z"
    }
   },
   "source": [
    "### 3.1 How it works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "AutoML performs three tasks, i.e. data preparation, model generation and Ensembles.\n",
    "\n",
    "**Data preparation** task includes mean imputing the missing values, standarising variables (therefore we have not standarised the variables in our data preparation task), one-hot encoding categorical features and features extraction (e.g. PCA).\n",
    "\n",
    "\n",
    "**model generation**: AutoML trains and cross-validates the following algorithms (in the following order): three pre-specified XGBoost GBM (Gradient Boosting Machine) models, a fixed grid of GLMs, a default Random Forest (DRF), five pre-specified H2O GBMs, a near-default Deep Neural Net, an Extremely Randomized Forest (XRT), a random grid of XGBoost GBMs, a random grid of H2O GBMs, and a random grid of Deep Neural Nets. In some cases, there will not be enough time to complete all the algorithms, so some may be missing from the leaderboard. This [source](https://www.slideshare.net/0xdata/intro-to-automl-handson-lab-erin-ledell-machine-learning-scientist-h2oai) also states that it uses Bayesian hyperparameter optimisation, but I couldn't find in the documentation as on which algorithm it actually uses this.\n",
    "\n",
    "\n",
    "**Ensembles**:Ensembles train Two Stacked models. One ensemble contains all the models, and the other ensemble provides just the best performing model from each algorithm class/family.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T17:35:28.269807Z",
     "start_time": "2020-07-16T17:35:28.262833Z"
    }
   },
   "source": [
    "\n",
    "* project_name: setting a name will enable us to train additional data on the same AutoML project\n",
    "* nfolds : H20 Documentation [here](https://www.slideshare.net/0xdata/scalable-automatic-machine-learning-in-h2o-89130971) suggest not to use leaderboard frame (validation set) unless it is necessary and suggest to go for the default method of nfolds where nfold = 5 and take 20% of the data.\n",
    "* exclude_algos: list of algorithm to exclude. We have not provided any value to it\n",
    "* balance_class: Specify whether to oversample the minority classes to balance the class distribution. But in our data, are balanced with both 0 and 1 having around 12,000 occurrence.\n",
    "\n",
    "The official document states \"The H2O AutoML interface is designed to have as few parameters as possible so that all the user needs to do is point to their dataset, identify the response column and optionally specify a time constraint or limit on the number of total models trained.\" Therefore, there are not many parameters to optimise. few of the parameters are discussed in the code as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters of GLM and XGBoost algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **GLM**: AutoML does not run a grid search for GLM. Instead AutoML builds a single model with lambda_search enabled and passes a list of alpha values. It returns only the model with the best alpha-lambda combination rather than one model for each alpha. alpha = {0.0, 0.2, 0.4, 0.6, 0.8, 1.0}. *Part 1 showed us that alpha value of 0.92 works well with our data but it is likely to be excluded in this.*\n",
    "\n",
    "* **XGBoost**: AutoML takes a variety of hyper parameters such as 'col_sample_rate', 'col_sample_rate_per_tree', 'max_depth', 'min_rows' etc. for random grid search but ntrees are set to be 10,000. This is quite odd and I didn't find the reason why they do it. Also I couldnt find the what value of learn_rate does this model takes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implementing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check if XGBoost is availbale. If its not then AutoML wont condier it\n",
    "h2o.estimators.xgboost.H2OXGBoostEstimator.available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  target</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\"> 127169</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\"> 122831</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if balance class is required\n",
    "h2o_df_train['target'].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify predictors and response\n",
    "x = h2o_df_train.columns\n",
    "y = \"target\"\n",
    "x.remove(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\n",
    "aml = H2OAutoML(max_models=20,\n",
    "                seed=2020, # setting the seeds are actually not recommended in the documentation, stating it will re-gerate\n",
    "                # the same result again, but here this is what we actually wants. By defaut is sets the sees randomly.\n",
    "                max_runtime_secs = 5400, # This argument specifies the maximum time that the AutoML process will run for,\n",
    "                # prior to training the final Stacked Ensemble models.\n",
    "                stopping_metric = 'AUC', # Auto is also equal to AUC\n",
    "                stopping_rounds = 3 # default\n",
    "                , exclude_algos = None\n",
    "                , export_checkpoints_dir = '/home/jovyan/Final_Project/PData')\n",
    "\n",
    "\n",
    "aml.train(x=x, y=y, training_frame=h2o_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                           </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_AllModels_AutoML_20200715_111507   </td><td style=\"text-align: right;\">0.881305</td><td style=\"text-align: right;\"> 0.436847</td><td style=\"text-align: right;\">              0.210478</td><td style=\"text-align: right;\">0.376244</td><td style=\"text-align: right;\">0.141559</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_AutoML_20200715_111507</td><td style=\"text-align: right;\">0.881037</td><td style=\"text-align: right;\"> 0.437242</td><td style=\"text-align: right;\">              0.211918</td><td style=\"text-align: right;\">0.376418</td><td style=\"text-align: right;\">0.141691</td></tr>\n",
       "<tr><td>GBM_5_AutoML_20200715_111507                       </td><td style=\"text-align: right;\">0.878721</td><td style=\"text-align: right;\"> 0.434454</td><td style=\"text-align: right;\">              0.217896</td><td style=\"text-align: right;\">0.376662</td><td style=\"text-align: right;\">0.141874</td></tr>\n",
       "<tr><td>XGBoost_1_AutoML_20200715_111507                   </td><td style=\"text-align: right;\">0.878474</td><td style=\"text-align: right;\"> 0.437477</td><td style=\"text-align: right;\">              0.214002</td><td style=\"text-align: right;\">0.377522</td><td style=\"text-align: right;\">0.142523</td></tr>\n",
       "<tr><td>XGBoost_2_AutoML_20200715_111507                   </td><td style=\"text-align: right;\">0.87813 </td><td style=\"text-align: right;\"> 0.445032</td><td style=\"text-align: right;\">              0.215206</td><td style=\"text-align: right;\">0.379171</td><td style=\"text-align: right;\">0.143771</td></tr>\n",
       "<tr><td>GBM_4_AutoML_20200715_111507                       </td><td style=\"text-align: right;\">0.878104</td><td style=\"text-align: right;\"> 0.435602</td><td style=\"text-align: right;\">              0.216831</td><td style=\"text-align: right;\">0.377158</td><td style=\"text-align: right;\">0.142248</td></tr>\n",
       "<tr><td>GBM_3_AutoML_20200715_111507                       </td><td style=\"text-align: right;\">0.878045</td><td style=\"text-align: right;\"> 0.435899</td><td style=\"text-align: right;\">              0.214013</td><td style=\"text-align: right;\">0.377295</td><td style=\"text-align: right;\">0.142351</td></tr>\n",
       "<tr><td>GBM_2_AutoML_20200715_111507                       </td><td style=\"text-align: right;\">0.87687 </td><td style=\"text-align: right;\"> 0.437741</td><td style=\"text-align: right;\">              0.216873</td><td style=\"text-align: right;\">0.378297</td><td style=\"text-align: right;\">0.143109</td></tr>\n",
       "<tr><td>GBM_1_AutoML_20200715_111507                       </td><td style=\"text-align: right;\">0.876527</td><td style=\"text-align: right;\"> 0.437983</td><td style=\"text-align: right;\">              0.216694</td><td style=\"text-align: right;\">0.378517</td><td style=\"text-align: right;\">0.143275</td></tr>\n",
       "<tr><td>DeepLearning_1_AutoML_20200715_111507              </td><td style=\"text-align: right;\">0.874498</td><td style=\"text-align: right;\"> 0.44315 </td><td style=\"text-align: right;\">              0.218672</td><td style=\"text-align: right;\">0.379649</td><td style=\"text-align: right;\">0.144133</td></tr>\n",
       "<tr><td>DRF_1_AutoML_20200715_111507                       </td><td style=\"text-align: right;\">0.867667</td><td style=\"text-align: right;\"> 0.464976</td><td style=\"text-align: right;\">              0.22449 </td><td style=\"text-align: right;\">0.387855</td><td style=\"text-align: right;\">0.150431</td></tr>\n",
       "<tr><td>XGBoost_3_AutoML_20200715_111507                   </td><td style=\"text-align: right;\">0.863623</td><td style=\"text-align: right;\"> 0.469994</td><td style=\"text-align: right;\">              0.227683</td><td style=\"text-align: right;\">0.391452</td><td style=\"text-align: right;\">0.153235</td></tr>\n",
       "<tr><td>GLM_grid_1_AutoML_20200715_111507_model_1          </td><td style=\"text-align: right;\">0.820114</td><td style=\"text-align: right;\"> 0.605626</td><td style=\"text-align: right;\">              0.27301 </td><td style=\"text-align: right;\">0.455204</td><td style=\"text-align: right;\">0.20721 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the algorithm result\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)\n",
    "#lb_all_columns = .get_leaderboard(aml, extra_columns = 'ALL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'a18' has levels not trained on: [D]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'c09' has levels not trained on: [nan]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'e03' has levels not trained on: [J, nan]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'e13' has levels not trained on: [Q, S]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'e24' has levels not trained on: [J, M, nan]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'e20' has levels not trained on: [30146, BE271, nan]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'f03_e11' has levels not trained on: [C_G]\n",
      "  warnings.warn(w)\n"
     ]
    }
   ],
   "source": [
    "# H20 Documentation says: To generate predictions on a test set, you can make predictions\n",
    "# directly on the `\"H2OAutoML\"` object or on the leader model\n",
    "# object directly\n",
    "\n",
    "preds = aml.leader.predict(h2o_df_test)\n",
    "df_test['Predicted'] = np.round(preds[2].as_data_frame(), 5)\n",
    "df_preds_dt = df_test[['unique_id', 'Predicted']].copy()\n",
    "df_test[['unique_id', 'Predicted']].to_csv(dirPOutput + 'AutoML_test)final.csv', index=False)\n",
    "\n",
    "# Kaggle socre = 0.860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_AutoML_20200715_111507\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.12175295314952604\n",
      "RMSE: 0.3489311581809885\n",
      "LogLoss: 0.3851042301810654\n",
      "Null degrees of freedom: 249999\n",
      "Residual degrees of freedom: 249993\n",
      "Null deviance: 346498.31352617935\n",
      "Residual deviance: 192552.1150905327\n",
      "AIC: 192566.1150905327\n",
      "AUC: 0.9118278376117468\n",
      "pr_auc: 0.9043886914211718\n",
      "Gini: 0.8236556752234936\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.43304635798588603: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>98520.0</td>\n",
       "<td>28649.0</td>\n",
       "<td>0.2253</td>\n",
       "<td> (28649.0/127169.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>16089.0</td>\n",
       "<td>106742.0</td>\n",
       "<td>0.131</td>\n",
       "<td> (16089.0/122831.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>114609.0</td>\n",
       "<td>135391.0</td>\n",
       "<td>0.179</td>\n",
       "<td> (44738.0/250000.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0       1       Error    Rate\n",
       "-----  ------  ------  -------  ------------------\n",
       "0      98520   28649   0.2253   (28649.0/127169.0)\n",
       "1      16089   106742  0.131    (16089.0/122831.0)\n",
       "Total  114609  135391  0.179    (44738.0/250000.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4330464</td>\n",
       "<td>0.8267460</td>\n",
       "<td>225.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1747967</td>\n",
       "<td>0.8926100</td>\n",
       "<td>327.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6806546</td>\n",
       "<td>0.8359927</td>\n",
       "<td>128.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5157484</td>\n",
       "<td>0.825052</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9417606</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0527699</td>\n",
       "<td>1.0</td>\n",
       "<td>396.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9417606</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5128156</td>\n",
       "<td>0.6500095</td>\n",
       "<td>194.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5128156</td>\n",
       "<td>0.8242952</td>\n",
       "<td>194.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5128156</td>\n",
       "<td>0.8250273</td>\n",
       "<td>194.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.433046     0.826746  225\n",
       "max f2                       0.174797     0.89261   327\n",
       "max f0point5                 0.680655     0.835993  128\n",
       "max accuracy                 0.515748     0.825052  193\n",
       "max precision                0.941761     1         0\n",
       "max recall                   0.0527699    1         396\n",
       "max specificity              0.941761     1         0\n",
       "max absolute_mcc             0.512816     0.65001   194\n",
       "max min_per_class_accuracy   0.512816     0.824295  194\n",
       "max mean_per_class_accuracy  0.512816     0.825027  194"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.13 %, avg score: 49.14 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.01</td>\n",
       "<td>0.9380719</td>\n",
       "<td>2.0345027</td>\n",
       "<td>2.0345027</td>\n",
       "<td>0.9996</td>\n",
       "<td>0.9398565</td>\n",
       "<td>0.9996</td>\n",
       "<td>0.9398565</td>\n",
       "<td>0.0203450</td>\n",
       "<td>0.0203450</td>\n",
       "<td>103.4502691</td>\n",
       "<td>103.4502691</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.02</td>\n",
       "<td>0.9358555</td>\n",
       "<td>2.0345027</td>\n",
       "<td>2.0345027</td>\n",
       "<td>0.9996</td>\n",
       "<td>0.9369448</td>\n",
       "<td>0.9996</td>\n",
       "<td>0.9384006</td>\n",
       "<td>0.0203450</td>\n",
       "<td>0.0406901</td>\n",
       "<td>103.4502691</td>\n",
       "<td>103.4502691</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.03</td>\n",
       "<td>0.9338231</td>\n",
       "<td>2.0296179</td>\n",
       "<td>2.0328744</td>\n",
       "<td>0.9972</td>\n",
       "<td>0.9348207</td>\n",
       "<td>0.9988</td>\n",
       "<td>0.9372073</td>\n",
       "<td>0.0202962</td>\n",
       "<td>0.0609862</td>\n",
       "<td>102.9617930</td>\n",
       "<td>103.2874437</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.04</td>\n",
       "<td>0.9318135</td>\n",
       "<td>2.0263614</td>\n",
       "<td>2.0312462</td>\n",
       "<td>0.9956</td>\n",
       "<td>0.9328115</td>\n",
       "<td>0.998</td>\n",
       "<td>0.9361083</td>\n",
       "<td>0.0202636</td>\n",
       "<td>0.0812498</td>\n",
       "<td>102.6361423</td>\n",
       "<td>103.1246184</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.05</td>\n",
       "<td>0.9298782</td>\n",
       "<td>2.0182202</td>\n",
       "<td>2.0286410</td>\n",
       "<td>0.9916</td>\n",
       "<td>0.9308554</td>\n",
       "<td>0.99672</td>\n",
       "<td>0.9350577</td>\n",
       "<td>0.0201822</td>\n",
       "<td>0.1014320</td>\n",
       "<td>101.8220156</td>\n",
       "<td>102.8640978</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1</td>\n",
       "<td>0.9183842</td>\n",
       "<td>2.0030774</td>\n",
       "<td>2.0158592</td>\n",
       "<td>0.98416</td>\n",
       "<td>0.9245022</td>\n",
       "<td>0.99044</td>\n",
       "<td>0.9297800</td>\n",
       "<td>0.1001539</td>\n",
       "<td>0.2015859</td>\n",
       "<td>100.3077399</td>\n",
       "<td>101.5859189</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.15</td>\n",
       "<td>0.9015113</td>\n",
       "<td>1.9537413</td>\n",
       "<td>1.9951532</td>\n",
       "<td>0.95992</td>\n",
       "<td>0.9104151</td>\n",
       "<td>0.9802667</td>\n",
       "<td>0.9233250</td>\n",
       "<td>0.0976871</td>\n",
       "<td>0.2992730</td>\n",
       "<td>95.3741319</td>\n",
       "<td>99.5153232</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.8773822</td>\n",
       "<td>1.8860060</td>\n",
       "<td>1.9678664</td>\n",
       "<td>0.92664</td>\n",
       "<td>0.8901701</td>\n",
       "<td>0.96686</td>\n",
       "<td>0.9150363</td>\n",
       "<td>0.0943003</td>\n",
       "<td>0.3935733</td>\n",
       "<td>88.6005976</td>\n",
       "<td>96.7866418</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3</td>\n",
       "<td>0.7960703</td>\n",
       "<td>1.7187843</td>\n",
       "<td>1.8848391</td>\n",
       "<td>0.84448</td>\n",
       "<td>0.8411968</td>\n",
       "<td>0.9260667</td>\n",
       "<td>0.8904231</td>\n",
       "<td>0.1718784</td>\n",
       "<td>0.5654517</td>\n",
       "<td>71.8784346</td>\n",
       "<td>88.4839061</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.6580722</td>\n",
       "<td>1.4611132</td>\n",
       "<td>1.7789076</td>\n",
       "<td>0.71788</td>\n",
       "<td>0.7307381</td>\n",
       "<td>0.87402</td>\n",
       "<td>0.8505018</td>\n",
       "<td>0.1461113</td>\n",
       "<td>0.7115630</td>\n",
       "<td>46.1113237</td>\n",
       "<td>77.8907605</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.5013498</td>\n",
       "<td>1.1898462</td>\n",
       "<td>1.6610953</td>\n",
       "<td>0.5846</td>\n",
       "<td>0.5796448</td>\n",
       "<td>0.816136</td>\n",
       "<td>0.7963304</td>\n",
       "<td>0.1189846</td>\n",
       "<td>0.8305477</td>\n",
       "<td>18.9846211</td>\n",
       "<td>66.1095326</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.3292029</td>\n",
       "<td>0.8391204</td>\n",
       "<td>1.5240995</td>\n",
       "<td>0.41228</td>\n",
       "<td>0.4162234</td>\n",
       "<td>0.7488267</td>\n",
       "<td>0.7329793</td>\n",
       "<td>0.0839120</td>\n",
       "<td>0.9144597</td>\n",
       "<td>-16.0879583</td>\n",
       "<td>52.4099508</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7</td>\n",
       "<td>0.1833846</td>\n",
       "<td>0.5386262</td>\n",
       "<td>1.3833176</td>\n",
       "<td>0.26464</td>\n",
       "<td>0.2508715</td>\n",
       "<td>0.6796571</td>\n",
       "<td>0.6641067</td>\n",
       "<td>0.0538626</td>\n",
       "<td>0.9683223</td>\n",
       "<td>-46.1373757</td>\n",
       "<td>38.3317613</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0975897</td>\n",
       "<td>0.2496113</td>\n",
       "<td>1.2416043</td>\n",
       "<td>0.12264</td>\n",
       "<td>0.1351597</td>\n",
       "<td>0.61003</td>\n",
       "<td>0.5979883</td>\n",
       "<td>0.0249611</td>\n",
       "<td>0.9932835</td>\n",
       "<td>-75.0388746</td>\n",
       "<td>24.1604318</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9</td>\n",
       "<td>0.0600105</td>\n",
       "<td>0.0638275</td>\n",
       "<td>1.1107402</td>\n",
       "<td>0.03136</td>\n",
       "<td>0.0756568</td>\n",
       "<td>0.5457333</td>\n",
       "<td>0.5399515</td>\n",
       "<td>0.0063828</td>\n",
       "<td>0.9996662</td>\n",
       "<td>-93.6172465</td>\n",
       "<td>11.0740231</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0488066</td>\n",
       "<td>0.0033379</td>\n",
       "<td>1.0</td>\n",
       "<td>0.00164</td>\n",
       "<td>0.0542567</td>\n",
       "<td>0.491324</td>\n",
       "<td>0.4913820</td>\n",
       "<td>0.0003338</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.6662080</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ----------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.01                        0.938072           2.0345      2.0345             0.9996           0.939856   0.9996                      0.939856            0.020345        0.020345                   103.45    103.45\n",
       "    2        0.02                        0.935856           2.0345      2.0345             0.9996           0.936945   0.9996                      0.938401            0.020345        0.0406901                  103.45    103.45\n",
       "    3        0.03                        0.933823           2.02962     2.03287            0.9972           0.934821   0.9988                      0.937207            0.0202962       0.0609862                  102.962   103.287\n",
       "    4        0.04                        0.931813           2.02636     2.03125            0.9956           0.932811   0.998                       0.936108            0.0202636       0.0812498                  102.636   103.125\n",
       "    5        0.05                        0.929878           2.01822     2.02864            0.9916           0.930855   0.99672                     0.935058            0.0201822       0.101432                   101.822   102.864\n",
       "    6        0.1                         0.918384           2.00308     2.01586            0.98416          0.924502   0.99044                     0.92978             0.100154        0.201586                   100.308   101.586\n",
       "    7        0.15                        0.901511           1.95374     1.99515            0.95992          0.910415   0.980267                    0.923325            0.0976871       0.299273                   95.3741   99.5153\n",
       "    8        0.2                         0.877382           1.88601     1.96787            0.92664          0.89017    0.96686                     0.915036            0.0943003       0.393573                   88.6006   96.7866\n",
       "    9        0.3                         0.79607            1.71878     1.88484            0.84448          0.841197   0.926067                    0.890423            0.171878        0.565452                   71.8784   88.4839\n",
       "    10       0.4                         0.658072           1.46111     1.77891            0.71788          0.730738   0.87402                     0.850502            0.146111        0.711563                   46.1113   77.8908\n",
       "    11       0.5                         0.50135            1.18985     1.6611             0.5846           0.579645   0.816136                    0.79633             0.118985        0.830548                   18.9846   66.1095\n",
       "    12       0.6                         0.329203           0.83912     1.5241             0.41228          0.416223   0.748827                    0.732979            0.083912        0.91446                    -16.088   52.41\n",
       "    13       0.7                         0.183385           0.538626    1.38332            0.26464          0.250871   0.679657                    0.664107            0.0538626       0.968322                   -46.1374  38.3318\n",
       "    14       0.8                         0.0975897          0.249611    1.2416             0.12264          0.13516    0.61003                     0.597988            0.0249611       0.993283                   -75.0389  24.1604\n",
       "    15       0.9                         0.0600105          0.0638275   1.11074            0.03136          0.0756568  0.545733                    0.539952            0.00638275      0.999666                   -93.6172  11.074\n",
       "    16       1                           0.0488066          0.00333792  1                  0.00164          0.0542567  0.491324                    0.491382            0.000333792     1                          -99.6662  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.1415594170010771\n",
      "RMSE: 0.37624382652885763\n",
      "LogLoss: 0.43684731351716816\n",
      "Null degrees of freedom: 249999\n",
      "Residual degrees of freedom: 249993\n",
      "Null deviance: 346499.73038801143\n",
      "Residual deviance: 218423.65675858408\n",
      "AIC: 218437.65675858408\n",
      "AUC: 0.8813045721676378\n",
      "pr_auc: 0.86987658922156\n",
      "Gini: 0.7626091443352756\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3815157721554832: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>91038.0</td>\n",
       "<td>36131.0</td>\n",
       "<td>0.2841</td>\n",
       "<td> (36131.0/127169.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>16808.0</td>\n",
       "<td>106023.0</td>\n",
       "<td>0.1368</td>\n",
       "<td> (16808.0/122831.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>107846.0</td>\n",
       "<td>142154.0</td>\n",
       "<td>0.2118</td>\n",
       "<td> (52939.0/250000.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0       1       Error    Rate\n",
       "-----  ------  ------  -------  ------------------\n",
       "0      91038   36131   0.2841   (36131.0/127169.0)\n",
       "1      16808   106023  0.1368   (16808.0/122831.0)\n",
       "Total  107846  142154  0.2118   (52939.0/250000.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3815158</td>\n",
       "<td>0.8002189</td>\n",
       "<td>247.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1340032</td>\n",
       "<td>0.8783951</td>\n",
       "<td>347.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6787119</td>\n",
       "<td>0.8031552</td>\n",
       "<td>130.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4914044</td>\n",
       "<td>0.792884</td>\n",
       "<td>204.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9382808</td>\n",
       "<td>0.9918490</td>\n",
       "<td>2.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0504010</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9411686</td>\n",
       "<td>0.9999450</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4334173</td>\n",
       "<td>0.5871186</td>\n",
       "<td>226.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5085504</td>\n",
       "<td>0.7920405</td>\n",
       "<td>197.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4653819</td>\n",
       "<td>0.7933155</td>\n",
       "<td>214.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.381516     0.800219  247\n",
       "max f2                       0.134003     0.878395  347\n",
       "max f0point5                 0.678712     0.803155  130\n",
       "max accuracy                 0.491404     0.792884  204\n",
       "max precision                0.938281     0.991849  2\n",
       "max recall                   0.050401     1         399\n",
       "max specificity              0.941169     0.999945  0\n",
       "max absolute_mcc             0.433417     0.587119  226\n",
       "max min_per_class_accuracy   0.50855      0.792041  197\n",
       "max mean_per_class_accuracy  0.465382     0.793316  214"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.13 %, avg score: 49.13 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.01</td>\n",
       "<td>0.9373213</td>\n",
       "<td>2.0182202</td>\n",
       "<td>2.0182202</td>\n",
       "<td>0.9916</td>\n",
       "<td>0.9392147</td>\n",
       "<td>0.9916</td>\n",
       "<td>0.9392147</td>\n",
       "<td>0.0201822</td>\n",
       "<td>0.0201822</td>\n",
       "<td>101.8220156</td>\n",
       "<td>101.8220156</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.02</td>\n",
       "<td>0.9350389</td>\n",
       "<td>2.0141495</td>\n",
       "<td>2.0161848</td>\n",
       "<td>0.9896</td>\n",
       "<td>0.9361284</td>\n",
       "<td>0.9906</td>\n",
       "<td>0.9376716</td>\n",
       "<td>0.0201415</td>\n",
       "<td>0.0403237</td>\n",
       "<td>101.4149523</td>\n",
       "<td>101.6184839</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.03</td>\n",
       "<td>0.9329862</td>\n",
       "<td>2.0100789</td>\n",
       "<td>2.0141495</td>\n",
       "<td>0.9876</td>\n",
       "<td>0.9340292</td>\n",
       "<td>0.9896</td>\n",
       "<td>0.9364575</td>\n",
       "<td>0.0201008</td>\n",
       "<td>0.0604245</td>\n",
       "<td>101.0078889</td>\n",
       "<td>101.4149523</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.04</td>\n",
       "<td>0.9310474</td>\n",
       "<td>1.9832127</td>\n",
       "<td>2.0064153</td>\n",
       "<td>0.9744</td>\n",
       "<td>0.9320226</td>\n",
       "<td>0.9858</td>\n",
       "<td>0.9353487</td>\n",
       "<td>0.0198321</td>\n",
       "<td>0.0802566</td>\n",
       "<td>98.3212707</td>\n",
       "<td>100.6415319</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.05</td>\n",
       "<td>0.9289912</td>\n",
       "<td>1.9840268</td>\n",
       "<td>2.0019376</td>\n",
       "<td>0.9748</td>\n",
       "<td>0.9300316</td>\n",
       "<td>0.9836</td>\n",
       "<td>0.9342853</td>\n",
       "<td>0.0198403</td>\n",
       "<td>0.1000969</td>\n",
       "<td>98.4026834</td>\n",
       "<td>100.1937622</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1</td>\n",
       "<td>0.9174043</td>\n",
       "<td>1.9420179</td>\n",
       "<td>1.9719778</td>\n",
       "<td>0.95416</td>\n",
       "<td>0.9235560</td>\n",
       "<td>0.96888</td>\n",
       "<td>0.9289206</td>\n",
       "<td>0.0971009</td>\n",
       "<td>0.1971978</td>\n",
       "<td>94.2017895</td>\n",
       "<td>97.1977758</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.15</td>\n",
       "<td>0.9003844</td>\n",
       "<td>1.8710260</td>\n",
       "<td>1.9383272</td>\n",
       "<td>0.91928</td>\n",
       "<td>0.9094049</td>\n",
       "<td>0.9523467</td>\n",
       "<td>0.9224154</td>\n",
       "<td>0.0935513</td>\n",
       "<td>0.2907491</td>\n",
       "<td>87.1026044</td>\n",
       "<td>93.8327187</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.8766146</td>\n",
       "<td>1.7844030</td>\n",
       "<td>1.8998461</td>\n",
       "<td>0.87672</td>\n",
       "<td>0.8891793</td>\n",
       "<td>0.93344</td>\n",
       "<td>0.9141064</td>\n",
       "<td>0.0892201</td>\n",
       "<td>0.3799692</td>\n",
       "<td>78.4402960</td>\n",
       "<td>89.9846130</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3</td>\n",
       "<td>0.7967872</td>\n",
       "<td>1.6311029</td>\n",
       "<td>1.8102651</td>\n",
       "<td>0.8014</td>\n",
       "<td>0.8409726</td>\n",
       "<td>0.8894267</td>\n",
       "<td>0.8897285</td>\n",
       "<td>0.1631103</td>\n",
       "<td>0.5430795</td>\n",
       "<td>63.1102897</td>\n",
       "<td>81.0265053</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.6599767</td>\n",
       "<td>1.4006236</td>\n",
       "<td>1.7078547</td>\n",
       "<td>0.68816</td>\n",
       "<td>0.7321379</td>\n",
       "<td>0.83911</td>\n",
       "<td>0.8503308</td>\n",
       "<td>0.1400624</td>\n",
       "<td>0.6831419</td>\n",
       "<td>40.0623621</td>\n",
       "<td>70.7854695</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.5000955</td>\n",
       "<td>1.1489770</td>\n",
       "<td>1.5960792</td>\n",
       "<td>0.56452</td>\n",
       "<td>0.5805255</td>\n",
       "<td>0.784192</td>\n",
       "<td>0.7963698</td>\n",
       "<td>0.1148977</td>\n",
       "<td>0.7980396</td>\n",
       "<td>14.8977050</td>\n",
       "<td>59.6079166</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.3271286</td>\n",
       "<td>0.9036807</td>\n",
       "<td>1.4806794</td>\n",
       "<td>0.444</td>\n",
       "<td>0.4136382</td>\n",
       "<td>0.7274933</td>\n",
       "<td>0.7325812</td>\n",
       "<td>0.0903681</td>\n",
       "<td>0.8884076</td>\n",
       "<td>-9.6319333</td>\n",
       "<td>48.0679416</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7</td>\n",
       "<td>0.1833546</td>\n",
       "<td>0.6162939</td>\n",
       "<td>1.3571958</td>\n",
       "<td>0.3028</td>\n",
       "<td>0.2509041</td>\n",
       "<td>0.6668229</td>\n",
       "<td>0.6637702</td>\n",
       "<td>0.0616294</td>\n",
       "<td>0.9500370</td>\n",
       "<td>-38.3706068</td>\n",
       "<td>35.7195775</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0982129</td>\n",
       "<td>0.3499931</td>\n",
       "<td>1.2312954</td>\n",
       "<td>0.17196</td>\n",
       "<td>0.1355185</td>\n",
       "<td>0.604965</td>\n",
       "<td>0.5977387</td>\n",
       "<td>0.0349993</td>\n",
       "<td>0.9850364</td>\n",
       "<td>-65.0006920</td>\n",
       "<td>23.1295438</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9</td>\n",
       "<td>0.0607731</td>\n",
       "<td>0.1320514</td>\n",
       "<td>1.1091572</td>\n",
       "<td>0.06488</td>\n",
       "<td>0.0765003</td>\n",
       "<td>0.5449556</td>\n",
       "<td>0.5398233</td>\n",
       "<td>0.0132051</td>\n",
       "<td>0.9982415</td>\n",
       "<td>-86.7948645</td>\n",
       "<td>10.9157207</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0485183</td>\n",
       "<td>0.0175851</td>\n",
       "<td>1.0</td>\n",
       "<td>0.00864</td>\n",
       "<td>0.0548368</td>\n",
       "<td>0.491324</td>\n",
       "<td>0.4913247</td>\n",
       "<td>0.0017585</td>\n",
       "<td>1.0</td>\n",
       "<td>-98.2414863</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.01                        0.937321           2.01822    2.01822            0.9916           0.939215   0.9916                      0.939215            0.0201822       0.0201822                  101.822   101.822\n",
       "    2        0.02                        0.935039           2.01415    2.01618            0.9896           0.936128   0.9906                      0.937672            0.0201415       0.0403237                  101.415   101.618\n",
       "    3        0.03                        0.932986           2.01008    2.01415            0.9876           0.934029   0.9896                      0.936457            0.0201008       0.0604245                  101.008   101.415\n",
       "    4        0.04                        0.931047           1.98321    2.00642            0.9744           0.932023   0.9858                      0.935349            0.0198321       0.0802566                  98.3213   100.642\n",
       "    5        0.05                        0.928991           1.98403    2.00194            0.9748           0.930032   0.9836                      0.934285            0.0198403       0.100097                   98.4027   100.194\n",
       "    6        0.1                         0.917404           1.94202    1.97198            0.95416          0.923556   0.96888                     0.928921            0.0971009       0.197198                   94.2018   97.1978\n",
       "    7        0.15                        0.900384           1.87103    1.93833            0.91928          0.909405   0.952347                    0.922415            0.0935513       0.290749                   87.1026   93.8327\n",
       "    8        0.2                         0.876615           1.7844     1.89985            0.87672          0.889179   0.93344                     0.914106            0.0892201       0.379969                   78.4403   89.9846\n",
       "    9        0.3                         0.796787           1.6311     1.81027            0.8014           0.840973   0.889427                    0.889728            0.16311         0.54308                    63.1103   81.0265\n",
       "    10       0.4                         0.659977           1.40062    1.70785            0.68816          0.732138   0.83911                     0.850331            0.140062        0.683142                   40.0624   70.7855\n",
       "    11       0.5                         0.500096           1.14898    1.59608            0.56452          0.580526   0.784192                    0.79637             0.114898        0.79804                    14.8977   59.6079\n",
       "    12       0.6                         0.327129           0.903681   1.48068            0.444            0.413638   0.727493                    0.732581            0.0903681       0.888408                   -9.63193  48.0679\n",
       "    13       0.7                         0.183355           0.616294   1.3572             0.3028           0.250904   0.666823                    0.66377             0.0616294       0.950037                   -38.3706  35.7196\n",
       "    14       0.8                         0.0982129          0.349993   1.2313             0.17196          0.135519   0.604965                    0.597739            0.0349993       0.985036                   -65.0007  23.1295\n",
       "    15       0.9                         0.0607731          0.132051   1.10916            0.06488          0.0765003  0.544956                    0.539823            0.0132051       0.998241                   -86.7949  10.9157\n",
       "    16       1                           0.0485183          0.0175851  1                  0.00864          0.0548368  0.491324                    0.491325            0.00175851      1                          -98.2415  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the leader algorithm\n",
    "aml.leader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Best Ensemble Exploration\n",
    "\n",
    "To understand how the ensemble works, let's take a peek inside the Stacked Ensemble \"All Models\" model. The \"All Models\" ensemble is an ensemble of all of the individual models in the AutoML run. This is the top performing model on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model ids for all models in the AutoML Leaderboard\n",
    "model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n",
    "# Get the \"All Models\" Stacked Ensemble model\n",
    "se = h2o.get_model([mid for mid in model_ids if \"StackedEnsemble_AllModels\" in mid][0])\n",
    "# Get the Stacked Ensemble metalearner model\n",
    "metalearner = h2o.get_model(se.metalearner()['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "metalearner.std_coef_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows us that XGboost followed by Deeplearning and GBM had the most weightage for our algorithm. Co-efficient of GLM is nearly zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGeneralizedLinearEstimator :  Generalized Linear Modeling\n",
      "Model Key:  metalearner_AUTO_StackedEnsemble_AllModels_AutoML_20200715_111507\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.1415503771610387\n",
      "RMSE: 0.3762318130634871\n",
      "LogLoss: 0.4368127330232597\n",
      "Null degrees of freedom: 249999\n",
      "Residual degrees of freedom: 249993\n",
      "Null deviance: 346498.31352617935\n",
      "Residual deviance: 218406.36651162984\n",
      "AIC: 218420.36651162984\n",
      "AUC: 0.8813244532896827\n",
      "pr_auc: 0.8705021026355652\n",
      "Gini: 0.7626489065793653\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.38303101077885854: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>91135.0</td>\n",
       "<td>36034.0</td>\n",
       "<td>0.2834</td>\n",
       "<td> (36034.0/127169.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>16883.0</td>\n",
       "<td>105948.0</td>\n",
       "<td>0.1374</td>\n",
       "<td> (16883.0/122831.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>108018.0</td>\n",
       "<td>141982.0</td>\n",
       "<td>0.2117</td>\n",
       "<td> (52917.0/250000.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0       1       Error    Rate\n",
       "-----  ------  ------  -------  ------------------\n",
       "0      91135   36034   0.2834   (36034.0/127169.0)\n",
       "1      16883   105948  0.1374   (16883.0/122831.0)\n",
       "Total  108018  141982  0.2117   (52917.0/250000.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3830310</td>\n",
       "<td>0.8001722</td>\n",
       "<td>246.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1358440</td>\n",
       "<td>0.8785517</td>\n",
       "<td>346.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6834284</td>\n",
       "<td>0.8031449</td>\n",
       "<td>130.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4946793</td>\n",
       "<td>0.793016</td>\n",
       "<td>205.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9415145</td>\n",
       "<td>0.9928058</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0502724</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9415145</td>\n",
       "<td>0.9999764</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4485215</td>\n",
       "<td>0.5871245</td>\n",
       "<td>222.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5088997</td>\n",
       "<td>0.7925117</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4657949</td>\n",
       "<td>0.7932980</td>\n",
       "<td>216.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.383031     0.800172  246\n",
       "max f2                       0.135844     0.878552  346\n",
       "max f0point5                 0.683428     0.803145  130\n",
       "max accuracy                 0.494679     0.793016  205\n",
       "max precision                0.941515     0.992806  0\n",
       "max recall                   0.0502724    1         399\n",
       "max specificity              0.941515     0.999976  0\n",
       "max absolute_mcc             0.448522     0.587124  222\n",
       "max min_per_class_accuracy   0.5089       0.792512  199\n",
       "max mean_per_class_accuracy  0.465795     0.793298  216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.13 %, avg score: 49.13 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.01</td>\n",
       "<td>0.9374579</td>\n",
       "<td>2.0182202</td>\n",
       "<td>2.0182202</td>\n",
       "<td>0.9916</td>\n",
       "<td>0.9393201</td>\n",
       "<td>0.9916</td>\n",
       "<td>0.9393201</td>\n",
       "<td>0.0201822</td>\n",
       "<td>0.0201822</td>\n",
       "<td>101.8220156</td>\n",
       "<td>101.8220156</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.02</td>\n",
       "<td>0.9351675</td>\n",
       "<td>2.0141495</td>\n",
       "<td>2.0161848</td>\n",
       "<td>0.9896</td>\n",
       "<td>0.9362495</td>\n",
       "<td>0.9906</td>\n",
       "<td>0.9377848</td>\n",
       "<td>0.0201415</td>\n",
       "<td>0.0403237</td>\n",
       "<td>101.4149523</td>\n",
       "<td>101.6184839</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.03</td>\n",
       "<td>0.9331187</td>\n",
       "<td>2.0125213</td>\n",
       "<td>2.0149636</td>\n",
       "<td>0.9888</td>\n",
       "<td>0.9341549</td>\n",
       "<td>0.99</td>\n",
       "<td>0.9365748</td>\n",
       "<td>0.0201252</td>\n",
       "<td>0.0604489</td>\n",
       "<td>101.2521269</td>\n",
       "<td>101.4963649</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.04</td>\n",
       "<td>0.9311676</td>\n",
       "<td>1.9807703</td>\n",
       "<td>2.0064153</td>\n",
       "<td>0.9732</td>\n",
       "<td>0.9321391</td>\n",
       "<td>0.9858</td>\n",
       "<td>0.9354659</td>\n",
       "<td>0.0198077</td>\n",
       "<td>0.0802566</td>\n",
       "<td>98.0770327</td>\n",
       "<td>100.6415319</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.05</td>\n",
       "<td>0.9291373</td>\n",
       "<td>1.9848410</td>\n",
       "<td>2.0021004</td>\n",
       "<td>0.9752</td>\n",
       "<td>0.9301513</td>\n",
       "<td>0.98368</td>\n",
       "<td>0.9344030</td>\n",
       "<td>0.0198484</td>\n",
       "<td>0.1001050</td>\n",
       "<td>98.4840960</td>\n",
       "<td>100.2100447</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1</td>\n",
       "<td>0.9175390</td>\n",
       "<td>1.9420179</td>\n",
       "<td>1.9720592</td>\n",
       "<td>0.95416</td>\n",
       "<td>0.9236772</td>\n",
       "<td>0.96892</td>\n",
       "<td>0.9290401</td>\n",
       "<td>0.0971009</td>\n",
       "<td>0.1972059</td>\n",
       "<td>94.2017895</td>\n",
       "<td>97.2059171</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.15</td>\n",
       "<td>0.9004814</td>\n",
       "<td>1.8713517</td>\n",
       "<td>1.9384900</td>\n",
       "<td>0.91944</td>\n",
       "<td>0.9095370</td>\n",
       "<td>0.9524267</td>\n",
       "<td>0.9225391</td>\n",
       "<td>0.0935676</td>\n",
       "<td>0.2907735</td>\n",
       "<td>87.1351695</td>\n",
       "<td>93.8490012</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.8767634</td>\n",
       "<td>1.7834260</td>\n",
       "<td>1.8997240</td>\n",
       "<td>0.87624</td>\n",
       "<td>0.8893168</td>\n",
       "<td>0.93338</td>\n",
       "<td>0.9142335</td>\n",
       "<td>0.0891713</td>\n",
       "<td>0.3799448</td>\n",
       "<td>78.3426008</td>\n",
       "<td>89.9724011</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3</td>\n",
       "<td>0.7969135</td>\n",
       "<td>1.6327312</td>\n",
       "<td>1.8107264</td>\n",
       "<td>0.8022</td>\n",
       "<td>0.8411107</td>\n",
       "<td>0.8896533</td>\n",
       "<td>0.8898592</td>\n",
       "<td>0.1632731</td>\n",
       "<td>0.5432179</td>\n",
       "<td>63.2731151</td>\n",
       "<td>81.0726391</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.6601018</td>\n",
       "<td>1.3999723</td>\n",
       "<td>1.7080379</td>\n",
       "<td>0.68784</td>\n",
       "<td>0.7322576</td>\n",
       "<td>0.8392</td>\n",
       "<td>0.8504588</td>\n",
       "<td>0.1399972</td>\n",
       "<td>0.6832151</td>\n",
       "<td>39.9972320</td>\n",
       "<td>70.8037873</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.5001571</td>\n",
       "<td>1.1475116</td>\n",
       "<td>1.5959326</td>\n",
       "<td>0.5638</td>\n",
       "<td>0.5805818</td>\n",
       "<td>0.78412</td>\n",
       "<td>0.7964834</td>\n",
       "<td>0.1147512</td>\n",
       "<td>0.7979663</td>\n",
       "<td>14.7511622</td>\n",
       "<td>59.5932623</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.3270140</td>\n",
       "<td>0.9039249</td>\n",
       "<td>1.4805980</td>\n",
       "<td>0.44412</td>\n",
       "<td>0.4135821</td>\n",
       "<td>0.7274533</td>\n",
       "<td>0.7326665</td>\n",
       "<td>0.0903925</td>\n",
       "<td>0.8883588</td>\n",
       "<td>-9.6075095</td>\n",
       "<td>48.0598003</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7</td>\n",
       "<td>0.1832172</td>\n",
       "<td>0.6178408</td>\n",
       "<td>1.3573470</td>\n",
       "<td>0.30356</td>\n",
       "<td>0.2507665</td>\n",
       "<td>0.6668971</td>\n",
       "<td>0.6638237</td>\n",
       "<td>0.0617841</td>\n",
       "<td>0.9501429</td>\n",
       "<td>-38.2159227</td>\n",
       "<td>35.7346970</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0980571</td>\n",
       "<td>0.3485277</td>\n",
       "<td>1.2312446</td>\n",
       "<td>0.17124</td>\n",
       "<td>0.1353681</td>\n",
       "<td>0.60494</td>\n",
       "<td>0.5977667</td>\n",
       "<td>0.0348528</td>\n",
       "<td>0.9849956</td>\n",
       "<td>-65.1472348</td>\n",
       "<td>23.1244556</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9</td>\n",
       "<td>0.0606572</td>\n",
       "<td>0.1320514</td>\n",
       "<td>1.1091120</td>\n",
       "<td>0.06488</td>\n",
       "<td>0.0763774</td>\n",
       "<td>0.5449333</td>\n",
       "<td>0.5398346</td>\n",
       "<td>0.0132051</td>\n",
       "<td>0.9982008</td>\n",
       "<td>-86.7948645</td>\n",
       "<td>10.9111978</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0485986</td>\n",
       "<td>0.0179922</td>\n",
       "<td>1.0</td>\n",
       "<td>0.00884</td>\n",
       "<td>0.0547308</td>\n",
       "<td>0.491324</td>\n",
       "<td>0.4913242</td>\n",
       "<td>0.0017992</td>\n",
       "<td>1.0</td>\n",
       "<td>-98.2007799</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.01                        0.937458           2.01822    2.01822            0.9916           0.93932    0.9916                      0.93932             0.0201822       0.0201822                  101.822   101.822\n",
       "    2        0.02                        0.935168           2.01415    2.01618            0.9896           0.936249   0.9906                      0.937785            0.0201415       0.0403237                  101.415   101.618\n",
       "    3        0.03                        0.933119           2.01252    2.01496            0.9888           0.934155   0.99                        0.936575            0.0201252       0.0604489                  101.252   101.496\n",
       "    4        0.04                        0.931168           1.98077    2.00642            0.9732           0.932139   0.9858                      0.935466            0.0198077       0.0802566                  98.077    100.642\n",
       "    5        0.05                        0.929137           1.98484    2.0021             0.9752           0.930151   0.98368                     0.934403            0.0198484       0.100105                   98.4841   100.21\n",
       "    6        0.1                         0.917539           1.94202    1.97206            0.95416          0.923677   0.96892                     0.92904             0.0971009       0.197206                   94.2018   97.2059\n",
       "    7        0.15                        0.900481           1.87135    1.93849            0.91944          0.909537   0.952427                    0.922539            0.0935676       0.290774                   87.1352   93.849\n",
       "    8        0.2                         0.876763           1.78343    1.89972            0.87624          0.889317   0.93338                     0.914234            0.0891713       0.379945                   78.3426   89.9724\n",
       "    9        0.3                         0.796914           1.63273    1.81073            0.8022           0.841111   0.889653                    0.889859            0.163273        0.543218                   63.2731   81.0726\n",
       "    10       0.4                         0.660102           1.39997    1.70804            0.68784          0.732258   0.8392                      0.850459            0.139997        0.683215                   39.9972   70.8038\n",
       "    11       0.5                         0.500157           1.14751    1.59593            0.5638           0.580582   0.78412                     0.796483            0.114751        0.797966                   14.7512   59.5933\n",
       "    12       0.6                         0.327014           0.903925   1.4806             0.44412          0.413582   0.727453                    0.732667            0.0903925       0.888359                   -9.60751  48.0598\n",
       "    13       0.7                         0.183217           0.617841   1.35735            0.30356          0.250766   0.666897                    0.663824            0.0617841       0.950143                   -38.2159  35.7347\n",
       "    14       0.8                         0.0980571          0.348528   1.23124            0.17124          0.135368   0.60494                     0.597767            0.0348528       0.984996                   -65.1472  23.1245\n",
       "    15       0.9                         0.0606572          0.132051   1.10911            0.06488          0.0763774  0.544933                    0.539835            0.0132051       0.998201                   -86.7949  10.9112\n",
       "    16       1                           0.0485986          0.0179922  1                  0.00884          0.0547308  0.491324                    0.491324            0.00179922      1                          -98.2008  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.1415594170010771\n",
      "RMSE: 0.37624382652885763\n",
      "LogLoss: 0.43684731351716816\n",
      "Null degrees of freedom: 249999\n",
      "Residual degrees of freedom: 249993\n",
      "Null deviance: 346499.73038801143\n",
      "Residual deviance: 218423.65675858408\n",
      "AIC: 218437.65675858408\n",
      "AUC: 0.8813045721676378\n",
      "pr_auc: 0.86987658922156\n",
      "Gini: 0.7626091443352756\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3815157721554832: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>91038.0</td>\n",
       "<td>36131.0</td>\n",
       "<td>0.2841</td>\n",
       "<td> (36131.0/127169.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>16808.0</td>\n",
       "<td>106023.0</td>\n",
       "<td>0.1368</td>\n",
       "<td> (16808.0/122831.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>107846.0</td>\n",
       "<td>142154.0</td>\n",
       "<td>0.2118</td>\n",
       "<td> (52939.0/250000.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0       1       Error    Rate\n",
       "-----  ------  ------  -------  ------------------\n",
       "0      91038   36131   0.2841   (36131.0/127169.0)\n",
       "1      16808   106023  0.1368   (16808.0/122831.0)\n",
       "Total  107846  142154  0.2118   (52939.0/250000.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3815158</td>\n",
       "<td>0.8002189</td>\n",
       "<td>247.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1340032</td>\n",
       "<td>0.8783951</td>\n",
       "<td>347.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6787119</td>\n",
       "<td>0.8031552</td>\n",
       "<td>130.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4914044</td>\n",
       "<td>0.792884</td>\n",
       "<td>204.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9382808</td>\n",
       "<td>0.9918490</td>\n",
       "<td>2.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0504010</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9411686</td>\n",
       "<td>0.9999450</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4334173</td>\n",
       "<td>0.5871186</td>\n",
       "<td>226.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5085504</td>\n",
       "<td>0.7920405</td>\n",
       "<td>197.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4653819</td>\n",
       "<td>0.7933155</td>\n",
       "<td>214.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.381516     0.800219  247\n",
       "max f2                       0.134003     0.878395  347\n",
       "max f0point5                 0.678712     0.803155  130\n",
       "max accuracy                 0.491404     0.792884  204\n",
       "max precision                0.938281     0.991849  2\n",
       "max recall                   0.050401     1         399\n",
       "max specificity              0.941169     0.999945  0\n",
       "max absolute_mcc             0.433417     0.587119  226\n",
       "max min_per_class_accuracy   0.50855      0.792041  197\n",
       "max mean_per_class_accuracy  0.465382     0.793316  214"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.13 %, avg score: 49.13 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.01</td>\n",
       "<td>0.9373213</td>\n",
       "<td>2.0182202</td>\n",
       "<td>2.0182202</td>\n",
       "<td>0.9916</td>\n",
       "<td>0.9392147</td>\n",
       "<td>0.9916</td>\n",
       "<td>0.9392147</td>\n",
       "<td>0.0201822</td>\n",
       "<td>0.0201822</td>\n",
       "<td>101.8220156</td>\n",
       "<td>101.8220156</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.02</td>\n",
       "<td>0.9350389</td>\n",
       "<td>2.0141495</td>\n",
       "<td>2.0161848</td>\n",
       "<td>0.9896</td>\n",
       "<td>0.9361284</td>\n",
       "<td>0.9906</td>\n",
       "<td>0.9376716</td>\n",
       "<td>0.0201415</td>\n",
       "<td>0.0403237</td>\n",
       "<td>101.4149523</td>\n",
       "<td>101.6184839</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.03</td>\n",
       "<td>0.9329862</td>\n",
       "<td>2.0100789</td>\n",
       "<td>2.0141495</td>\n",
       "<td>0.9876</td>\n",
       "<td>0.9340292</td>\n",
       "<td>0.9896</td>\n",
       "<td>0.9364575</td>\n",
       "<td>0.0201008</td>\n",
       "<td>0.0604245</td>\n",
       "<td>101.0078889</td>\n",
       "<td>101.4149523</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.04</td>\n",
       "<td>0.9310474</td>\n",
       "<td>1.9832127</td>\n",
       "<td>2.0064153</td>\n",
       "<td>0.9744</td>\n",
       "<td>0.9320226</td>\n",
       "<td>0.9858</td>\n",
       "<td>0.9353487</td>\n",
       "<td>0.0198321</td>\n",
       "<td>0.0802566</td>\n",
       "<td>98.3212707</td>\n",
       "<td>100.6415319</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.05</td>\n",
       "<td>0.9289912</td>\n",
       "<td>1.9840268</td>\n",
       "<td>2.0019376</td>\n",
       "<td>0.9748</td>\n",
       "<td>0.9300316</td>\n",
       "<td>0.9836</td>\n",
       "<td>0.9342853</td>\n",
       "<td>0.0198403</td>\n",
       "<td>0.1000969</td>\n",
       "<td>98.4026834</td>\n",
       "<td>100.1937622</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1</td>\n",
       "<td>0.9174043</td>\n",
       "<td>1.9420179</td>\n",
       "<td>1.9719778</td>\n",
       "<td>0.95416</td>\n",
       "<td>0.9235560</td>\n",
       "<td>0.96888</td>\n",
       "<td>0.9289206</td>\n",
       "<td>0.0971009</td>\n",
       "<td>0.1971978</td>\n",
       "<td>94.2017895</td>\n",
       "<td>97.1977758</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.15</td>\n",
       "<td>0.9003844</td>\n",
       "<td>1.8710260</td>\n",
       "<td>1.9383272</td>\n",
       "<td>0.91928</td>\n",
       "<td>0.9094049</td>\n",
       "<td>0.9523467</td>\n",
       "<td>0.9224154</td>\n",
       "<td>0.0935513</td>\n",
       "<td>0.2907491</td>\n",
       "<td>87.1026044</td>\n",
       "<td>93.8327187</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.8766146</td>\n",
       "<td>1.7844030</td>\n",
       "<td>1.8998461</td>\n",
       "<td>0.87672</td>\n",
       "<td>0.8891793</td>\n",
       "<td>0.93344</td>\n",
       "<td>0.9141064</td>\n",
       "<td>0.0892201</td>\n",
       "<td>0.3799692</td>\n",
       "<td>78.4402960</td>\n",
       "<td>89.9846130</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3</td>\n",
       "<td>0.7967872</td>\n",
       "<td>1.6311029</td>\n",
       "<td>1.8102651</td>\n",
       "<td>0.8014</td>\n",
       "<td>0.8409726</td>\n",
       "<td>0.8894267</td>\n",
       "<td>0.8897285</td>\n",
       "<td>0.1631103</td>\n",
       "<td>0.5430795</td>\n",
       "<td>63.1102897</td>\n",
       "<td>81.0265053</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.6599767</td>\n",
       "<td>1.4006236</td>\n",
       "<td>1.7078547</td>\n",
       "<td>0.68816</td>\n",
       "<td>0.7321379</td>\n",
       "<td>0.83911</td>\n",
       "<td>0.8503308</td>\n",
       "<td>0.1400624</td>\n",
       "<td>0.6831419</td>\n",
       "<td>40.0623621</td>\n",
       "<td>70.7854695</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.5000955</td>\n",
       "<td>1.1489770</td>\n",
       "<td>1.5960792</td>\n",
       "<td>0.56452</td>\n",
       "<td>0.5805255</td>\n",
       "<td>0.784192</td>\n",
       "<td>0.7963698</td>\n",
       "<td>0.1148977</td>\n",
       "<td>0.7980396</td>\n",
       "<td>14.8977050</td>\n",
       "<td>59.6079166</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.3271286</td>\n",
       "<td>0.9036807</td>\n",
       "<td>1.4806794</td>\n",
       "<td>0.444</td>\n",
       "<td>0.4136382</td>\n",
       "<td>0.7274933</td>\n",
       "<td>0.7325812</td>\n",
       "<td>0.0903681</td>\n",
       "<td>0.8884076</td>\n",
       "<td>-9.6319333</td>\n",
       "<td>48.0679416</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7</td>\n",
       "<td>0.1833546</td>\n",
       "<td>0.6162939</td>\n",
       "<td>1.3571958</td>\n",
       "<td>0.3028</td>\n",
       "<td>0.2509041</td>\n",
       "<td>0.6668229</td>\n",
       "<td>0.6637702</td>\n",
       "<td>0.0616294</td>\n",
       "<td>0.9500370</td>\n",
       "<td>-38.3706068</td>\n",
       "<td>35.7195775</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0982129</td>\n",
       "<td>0.3499931</td>\n",
       "<td>1.2312954</td>\n",
       "<td>0.17196</td>\n",
       "<td>0.1355185</td>\n",
       "<td>0.604965</td>\n",
       "<td>0.5977387</td>\n",
       "<td>0.0349993</td>\n",
       "<td>0.9850364</td>\n",
       "<td>-65.0006920</td>\n",
       "<td>23.1295438</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9</td>\n",
       "<td>0.0607731</td>\n",
       "<td>0.1320514</td>\n",
       "<td>1.1091572</td>\n",
       "<td>0.06488</td>\n",
       "<td>0.0765003</td>\n",
       "<td>0.5449556</td>\n",
       "<td>0.5398233</td>\n",
       "<td>0.0132051</td>\n",
       "<td>0.9982415</td>\n",
       "<td>-86.7948645</td>\n",
       "<td>10.9157207</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0485183</td>\n",
       "<td>0.0175851</td>\n",
       "<td>1.0</td>\n",
       "<td>0.00864</td>\n",
       "<td>0.0548368</td>\n",
       "<td>0.491324</td>\n",
       "<td>0.4913247</td>\n",
       "<td>0.0017585</td>\n",
       "<td>1.0</td>\n",
       "<td>-98.2414863</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.01                        0.937321           2.01822    2.01822            0.9916           0.939215   0.9916                      0.939215            0.0201822       0.0201822                  101.822   101.822\n",
       "    2        0.02                        0.935039           2.01415    2.01618            0.9896           0.936128   0.9906                      0.937672            0.0201415       0.0403237                  101.415   101.618\n",
       "    3        0.03                        0.932986           2.01008    2.01415            0.9876           0.934029   0.9896                      0.936457            0.0201008       0.0604245                  101.008   101.415\n",
       "    4        0.04                        0.931047           1.98321    2.00642            0.9744           0.932023   0.9858                      0.935349            0.0198321       0.0802566                  98.3213   100.642\n",
       "    5        0.05                        0.928991           1.98403    2.00194            0.9748           0.930032   0.9836                      0.934285            0.0198403       0.100097                   98.4027   100.194\n",
       "    6        0.1                         0.917404           1.94202    1.97198            0.95416          0.923556   0.96888                     0.928921            0.0971009       0.197198                   94.2018   97.1978\n",
       "    7        0.15                        0.900384           1.87103    1.93833            0.91928          0.909405   0.952347                    0.922415            0.0935513       0.290749                   87.1026   93.8327\n",
       "    8        0.2                         0.876615           1.7844     1.89985            0.87672          0.889179   0.93344                     0.914106            0.0892201       0.379969                   78.4403   89.9846\n",
       "    9        0.3                         0.796787           1.6311     1.81027            0.8014           0.840973   0.889427                    0.889728            0.16311         0.54308                    63.1103   81.0265\n",
       "    10       0.4                         0.659977           1.40062    1.70785            0.68816          0.732138   0.83911                     0.850331            0.140062        0.683142                   40.0624   70.7855\n",
       "    11       0.5                         0.500096           1.14898    1.59608            0.56452          0.580526   0.784192                    0.79637             0.114898        0.79804                    14.8977   59.6079\n",
       "    12       0.6                         0.327129           0.903681   1.48068            0.444            0.413638   0.727493                    0.732581            0.0903681       0.888408                   -9.63193  48.0679\n",
       "    13       0.7                         0.183355           0.616294   1.3572             0.3028           0.250904   0.666823                    0.66377             0.0616294       0.950037                   -38.3706  35.7196\n",
       "    14       0.8                         0.0982129          0.349993   1.2313             0.17196          0.135519   0.604965                    0.597739            0.0349993       0.985036                   -65.0007  23.1295\n",
       "    15       0.9                         0.0607731          0.132051   1.10916            0.06488          0.0765003  0.544956                    0.539823            0.0132051       0.998241                   -86.7949  10.9157\n",
       "    16       1                           0.0485183          0.0175851  1                  0.00864          0.0548368  0.491324                    0.491325            0.00175851      1                          -98.2415  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Metrics Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>mean</b></td>\n",
       "<td><b>sd</b></td>\n",
       "<td><b>cv_1_valid</b></td>\n",
       "<td><b>cv_2_valid</b></td>\n",
       "<td><b>cv_3_valid</b></td>\n",
       "<td><b>cv_4_valid</b></td>\n",
       "<td><b>cv_5_valid</b></td></tr>\n",
       "<tr><td>accuracy</td>\n",
       "<td>0.7857159</td>\n",
       "<td>0.0021270</td>\n",
       "<td>0.7879333</td>\n",
       "<td>0.7812162</td>\n",
       "<td>0.7851982</td>\n",
       "<td>0.7842972</td>\n",
       "<td>0.7899345</td></tr>\n",
       "<tr><td>auc</td>\n",
       "<td>0.8813058</td>\n",
       "<td>0.0008034</td>\n",
       "<td>0.8812296</td>\n",
       "<td>0.8801973</td>\n",
       "<td>0.8827597</td>\n",
       "<td>0.8799323</td>\n",
       "<td>0.8824099</td></tr>\n",
       "<tr><td>err</td>\n",
       "<td>0.2142841</td>\n",
       "<td>0.0021270</td>\n",
       "<td>0.2120667</td>\n",
       "<td>0.2187838</td>\n",
       "<td>0.2148018</td>\n",
       "<td>0.2157028</td>\n",
       "<td>0.2100655</td></tr>\n",
       "<tr><td>err_count</td>\n",
       "<td>10714.2</td>\n",
       "<td>107.60428</td>\n",
       "<td>10608.0</td>\n",
       "<td>10923.0</td>\n",
       "<td>10791.0</td>\n",
       "<td>10764.0</td>\n",
       "<td>10485.0</td></tr>\n",
       "<tr><td>f0point5</td>\n",
       "<td>0.7615901</td>\n",
       "<td>0.0032423</td>\n",
       "<td>0.7656216</td>\n",
       "<td>0.754774</td>\n",
       "<td>0.7600142</td>\n",
       "<td>0.7598704</td>\n",
       "<td>0.7676703</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>r2</td>\n",
       "<td>0.4335817</td>\n",
       "<td>0.0018254</td>\n",
       "<td>0.4331871</td>\n",
       "<td>0.4312677</td>\n",
       "<td>0.4371522</td>\n",
       "<td>0.4304407</td>\n",
       "<td>0.4358610</td></tr>\n",
       "<tr><td>recall</td>\n",
       "<td>0.8754463</td>\n",
       "<td>0.0071936</td>\n",
       "<td>0.8654175</td>\n",
       "<td>0.8863664</td>\n",
       "<td>0.8835772</td>\n",
       "<td>0.8807038</td>\n",
       "<td>0.8611669</td></tr>\n",
       "<tr><td>residual_deviance</td>\n",
       "<td>43684.73</td>\n",
       "<td>92.08617</td>\n",
       "<td>43718.13</td>\n",
       "<td>43782.633</td>\n",
       "<td>43653.65</td>\n",
       "<td>43819.164</td>\n",
       "<td>43450.082</td></tr>\n",
       "<tr><td>rmse</td>\n",
       "<td>0.3762443</td>\n",
       "<td>0.0006124</td>\n",
       "<td>0.3763709</td>\n",
       "<td>0.3770163</td>\n",
       "<td>0.3750747</td>\n",
       "<td>0.3773107</td>\n",
       "<td>0.3754489</td></tr>\n",
       "<tr><td>specificity</td>\n",
       "<td>0.6989823</td>\n",
       "<td>0.0112306</td>\n",
       "<td>0.7132538</td>\n",
       "<td>0.6796109</td>\n",
       "<td>0.6896944</td>\n",
       "<td>0.6904828</td>\n",
       "<td>0.7218696</td></tr></table></div>"
      ],
      "text/plain": [
       "                   mean        sd            cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
       "-----------------  ----------  ------------  ------------  ------------  ------------  ------------  ------------\n",
       "accuracy           0.7857159   0.002127002   0.7879333     0.7812162     0.78519815    0.7842972     0.78993446\n",
       "auc                0.8813058   8.0336444E-4  0.88122964    0.88019735    0.88275975    0.87993234    0.88240993\n",
       "err                0.21428412  0.002127002   0.2120667     0.2187838     0.21480183    0.21570277    0.21006551\n",
       "err_count          10714.2     107.60428     10608.0       10923.0       10791.0       10764.0       10485.0\n",
       "f0point5           0.7615901   0.0032422706  0.7656216     0.754774      0.7600142     0.7598704     0.76767033\n",
       "---                ---         ---           ---           ---           ---           ---           ---\n",
       "r2                 0.4335817   0.0018253692  0.4331871     0.43126768    0.43715218    0.43044066    0.43586096\n",
       "recall             0.8754463   0.0071935914  0.86541754    0.8863664     0.88357717    0.88070375    0.8611669\n",
       "residual_deviance  43684.73    92.08617      43718.13      43782.633     43653.65      43819.164     43450.082\n",
       "rmse               0.3762443   6.1237806E-4  0.37637094    0.37701628    0.37507468    0.37731075    0.37544885\n",
       "specificity        0.6989823   0.0112305945  0.7132538     0.6796109     0.6896944     0.6904828     0.7218696"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>iterations</b></td>\n",
       "<td><b>negative_log_likelihood</b></td>\n",
       "<td><b>objective</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-15 12:45:33</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>0</td>\n",
       "<td>173249.1567631</td>\n",
       "<td>0.6929966</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-15 12:45:33</td>\n",
       "<td> 0.103 sec</td>\n",
       "<td>1</td>\n",
       "<td>113005.5595574</td>\n",
       "<td>0.4525288</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-15 12:45:33</td>\n",
       "<td> 0.153 sec</td>\n",
       "<td>2</td>\n",
       "<td>109345.7864279</td>\n",
       "<td>0.4380721</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-15 12:45:33</td>\n",
       "<td> 0.208 sec</td>\n",
       "<td>3</td>\n",
       "<td>109203.9393470</td>\n",
       "<td>0.4375531</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-15 12:45:33</td>\n",
       "<td> 0.259 sec</td>\n",
       "<td>4</td>\n",
       "<td>109203.1832558</td>\n",
       "<td>0.4375520</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    iterations    negative_log_likelihood    objective\n",
       "--  -------------------  ----------  ------------  -------------------------  -----------\n",
       "    2020-07-15 12:45:33  0.000 sec   0             173249                     0.692997\n",
       "    2020-07-15 12:45:33  0.103 sec   1             113006                     0.452529\n",
       "    2020-07-15 12:45:33  0.153 sec   2             109346                     0.438072\n",
       "    2020-07-15 12:45:33  0.208 sec   3             109204                     0.437553\n",
       "    2020-07-15 12:45:33  0.259 sec   4             109203                     0.437552"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metalearner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
